---
title: "Final Project"
author: "Feiyu Zheng (fz114) & Junyi Li()"
date: "2022/3/27"
output:
  pdf_document: default
  html_document:
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(tidyverse)
library(lubridate)
library(ggplot2)
library(tidytext)
library(textdata)
library(rvest)
library(httr)
library(curl)
library(jsonlite)
library(randomForest)
library(caret)

# suppress the warning message of dplyr
options(dplyr.summarise.inform = F)
```

# Dataset

## Download source data from kaggle

```{r warning=FALSE}
kaggle <- fromJSON("kaggle.json")

if(!dir.exists("./Data")){
  dir.create("./Data")
}
if(!file.exists("./Data/train.csv")){
  data <- httr::GET("https://www.kaggle.com/api/v1/competitions/data/download/17777/train.csv", httr::authenticate(kaggle$username, kaggle$key, type = "basic"))
  download.file(data$url, "./Data/train.csv")
  data <- httr::GET("https://www.kaggle.com/api/v1/competitions/data/download/17777/test.csv", httr::authenticate(kaggle$username, kaggle$key, type = "basic"))
  download.file(data$url, "./Data/test.csv")
}
```

## Read downloaded csv files and convert them to tibble

```{r}
encoding <- "UTF-8"
trainset <- read.csv(
  "./Data/train.csv",
  encoding = encoding,
  header = T
) %>%
  as_tibble()

testset <- read.csv(
  "./Data/test_result.csv",
  encoding = encoding,
  header = T
) %>%
  as_tibble()

# result_set <- read.csv(
#   "./Data/socialmedia-disaster-tweets-DFE.csv",
#   encoding = encoding,
#   header = T
# ) %>%
#   as_tibble()

trainset
testset
# result_set %>%
#   mutate(id = row_number() - 1) %>%
#   select(id, text, choose_one) %>%
#   inner_join(testset, by = 'id') %>%
#   select(id, keyword, location, text.y, choose_one) %>%
#   rename(text = text.y, target = choose_one) %>%
#   mutate(target = case_when(
#     target == "Relevant" ~ as.integer(1),
#     T ~ as.integer(0)
#   )) %>%
#   write.csv(file = "./Data/test_result.csv", row.names = F)
```

## 

# Data Preprocess

## Simple analysis on the dataset

```{r}
trainsetRatio <- trainset %>%
  mutate(total = n()) %>%
  group_by(target, total) %>%
  summarise(count = n()) %>%
  mutate(ratio = round(count / total, 2)) %>%
  select(target, count, total, ratio) %>%
  ungroup()
trainsetRatio
```

## Dataset split according to target ratio

```{r}
total <- nrow(trainset)
zeroRatio <- trainsetRatio %>%
  filter(target == 0) %>%
  pull(ratio)
oneRatio <- trainsetRatio %>%
  filter(target == 1) %>%
  pull(ratio)

# set random seed for sample
set.seed(25)

# extract 20% data from trainset as validset
valRatio <- 0.2
zero <- trainset %>%
  filter(target == 0) %>%
  sample_n(zeroRatio * total * valRatio)
one <- trainset %>%
  filter(target == 1) %>%
  sample_n(oneRatio * total * valRatio)
valset <- bind_rows(
  sample_n(
    filter(trainset, target == 0),
    zeroRatio * total * valRatio
  ),
  sample_n(
    filter(trainset, target == 1),
    oneRatio * total * valRatio
  )
) %>%
  arrange(id)

# exclude those validset data from trainset
trainset <- trainset %>%
  anti_join(valset, by = "id") %>%
  arrange(id)

trainset
valset
```

## Replace the whitespace unicode %20 in *keywords* variable and newline character \n in text

```{r}
replace_space_unicode <- function(dataset){
  dataset %>%
    mutate(
      keyword = str_replace_all(keyword, "%20", " "),
      text = str_replace_all(text, "\n", " ")
    ) %>%
    mutate(
      keyword = na_if(keyword, "")
    )
}
replace_new_line <- function(dataset){
  dataset %>% mutate(
    text = str_replace_all(text, "\n", " ")
  )
}
tidy_trainset <- trainset %>%
  replace_space_unicode() %>%
  replace_new_line()
tidy_trainset

tidy_valid <- valset %>%
  replace_space_unicode() %>%
  replace_new_line()

tidy_test <- testset %>%
  replace_space_unicode() %>%
  replace_new_line()

tidy_trainset %>%
  group_by(keyword) %>%
  count() %>%
  arrange(desc(n)) %>%
  ungroup()
```

## Extract Tags, '\@'s and links in *text* variable

```{r}
extract_at <- function(dataset){
  dataset %>%
    mutate(
      at = str_replace_all(
        map_chr(
          map(
            .x = str_extract_all(tolower(.$text), "@\\S+"),
            unlist
          ),
          ~case_when(
            paste(.x, collapse = ',') == '' ~ NA_character_,
            T ~ paste(.x, collapse = ',')
          )
        ),
        '@',
        ''
      ),
      at_count = str_count(tolower(.$text), "@\\S+"),
      text = str_replace_all(text, "@\\S+", '')
    )
}
extract_links <- function(dataset){
  dataset %>%
    mutate(
      links = map_chr(
        map(
          .x = str_extract_all(tolower(.$text), '\\S+\\.\\S+/\\S\\s*'),
          unlist
        ),
        ~case_when(
          paste(.x, collapse = ',') == '' ~ NA_character_,
          T ~ paste(.x, collapse = ',')
        )
      ),
      link_count = str_count(tolower(.$text), '\\S+\\.\\S+/\\S\\s*'),
      text = str_replace_all(text, "\\S+//\\S+\\.?\\S+\\S\\s*", '')
    )
}
extract_tags <- function(dataset){
  dataset %>%
    mutate(
      tags = str_replace_all(
        map_chr(
          map(
            .x = str_extract_all(tolower(.$text), "#\\S+"),
            unlist
          ),
          ~case_when(
            paste(.x, collapse = ',') == '' ~ NA_character_,
            T ~ paste(.x, collapse = ',')
          )
        ),
        '#',
        ''
      ),
      tag_count = str_count(tolower(.$text), "#\\S+"),
      text = str_replace_all(text, "#", '')
    )
}
tidy_trainset <- tidy_trainset %>%
  extract_tags() %>%
  extract_at() %>%
  extract_links()
tidy_trainset

tidy_valid <- tidy_valid %>%
  extract_tags() %>%
  extract_at() %>%
  extract_links()

tidy_test <- tidy_test %>%
  extract_tags() %>%
  extract_at() %>%
  extract_links()
```

## Compute the length of each *text* variable

```{r}
extract_char_length <- function(dataset){
  dataset %>%
    mutate(char_length = nchar(.$text))
}
extract_word_count <- function(dataset){
  dataset %>%
    mutate(word_count = str_count(text, "\\w+"))
}
extract_mean_word_length <- function(dataset){
  dataset %>%
    mutate(mean_word_length = char_length/word_count)
}
extract_unique_word_count <- function(dataset){
  dataset %>%
    mutate(
      unique_word_count = map_int(
        .$text,
        ~ .x %>% str_extract_all("\\w+") %>% unlist %>% n_distinct
      )
    )
}
extract_puncuation_count <- function(dataset){
  dataset %>%
    mutate(
      punc_count = str_count(text, "[[:punct:]]+")
    )
}
extract_stop_word_count <- function(dataset){
  dataset %>%
  mutate(
    stop_word_count = map_int(
      text,
      ~ .x %>%
        tolower() %>%
        str_split(' ') %>%
        map(., ~case_when(. %in% stop_words$word ~ T, T ~ F)) %>%
        unlist %>%
        sum()
    )
  )
}

classify_char_length <- function(dataset){
  dataset %>%
    mutate(
      char_length_type = case_when(
        char_length <= 50 ~ "short",
        char_length <= 100 ~ "medium",
        char_length <= 150 ~ "long",
        T ~ "other"
      )
    )
}
classify_word_length <- function(dataset){
  dataset %>%
    mutate(
      word_count_type = case_when(
        word_count <= 5 ~ "0-5",
        word_count <= 10 ~ "5-10",
        word_count <= 15 ~ "10-15",
        word_count <= 20 ~ "15-20",
        word_count <= 25 ~ "20-25",
        word_count <= 30 ~ "25-30",
        T ~ "other"
      )
    )
}

tidy_trainset <- tidy_trainset %>%
  extract_char_length() %>%
  extract_word_count() %>%
  extract_mean_word_length() %>%
  extract_unique_word_count() %>%
  extract_puncuation_count() %>%
  extract_stop_word_count()

tidy_valid <- tidy_valid %>%
  extract_char_length() %>%
  extract_word_count() %>%
  extract_mean_word_length() %>%
  extract_unique_word_count() %>%
  extract_puncuation_count() %>%
  extract_stop_word_count()

tidy_test <- tidy_test %>%
  extract_char_length() %>%
  extract_word_count() %>%
  extract_mean_word_length() %>%
  extract_unique_word_count() %>%
  extract_puncuation_count() %>%
  extract_stop_word_count()

tidy_trainset <- tidy_trainset %>%
  classify_char_length() %>%
  classify_word_length()

tidy_valid <- tidy_valid %>%
  classify_char_length() %>%
  classify_word_length()

tidy_test <- tidy_test %>%
  classify_char_length() %>%
  classify_word_length()

tidy_trainset %>%
  mutate(target = as.factor(target)) %>%
  ggplot() +
  geom_bar(aes(x = char_length_type, fill = target)) +
  labs(
    x = "Length Type",
    y = "Count",
    title = "Distribution of Length Types"
  ) +
  theme(plot.title = element_text(hjust = 0.5))

tidy_trainset %>%
  mutate(target = as.factor(target)) %>%
  ggplot() +
  geom_histogram(aes(x = word_count, fill = as.factor(target))) +
  labs(
    x = "word count",
    y = "Count",
    title = "Distribution of word count"
  ) +
  theme(plot.title = element_text(hjust = 0.5))

tidy_trainset %>%
  mutate(target = as.factor(target)) %>%
  ggplot() +
  geom_bar(aes(x = word_count_type, fill = target)) +
  labs(
    x = "Word Count Type",
    y = "Count",
    title = "Distribution of Length Types"
  ) +
  theme(plot.title = element_text(hjust = 0.5))

tidy_trainset
```

## Ratio of each keyword vs target

```{r}
keyword_ratio <- tidy_trainset %>%
  group_by(keyword, target) %>%
  summarise(n = n()) %>%
  mutate(ratio = n / sum(n)) %>%
  ungroup() %>%
  arrange(desc(n), desc(ratio))
keyword_ratio
```

## Create disaster keyword

```{r}
count_threshold <- 20
ratio_threshold <- 0.8
disaster_keywords <- keyword_ratio %>%
  filter(n >= count_threshold & target == 1 & ratio >= ratio_threshold) %>%
  .$keyword

checkDisasterKeyword <- function(dataset, disaster_keywords){
  dataset %>%
    mutate(
      contains_disaster_keyword = str_detect(
        keyword,
        paste(disaster_keywords, collapse = "|")
      )
    ) %>%
    mutate(contains_disaster_keyword = case_when(
      contains_disaster_keyword == T ~ "True",
      contains_disaster_keyword == F ~ "False",
      T ~ "Empty"
    ))
}

tidy_trainset <- checkDisasterKeyword(tidy_trainset, disaster_keywords)
  # tidy_trainset %>%
  # mutate(
  #   containsDisasterKeyword = str_detect(
  #     keyword,
  #     paste(disasterKeywords, collapse = "|")
  #   )
  # ) %>%
  # mutate(containsDisasterKeyword = case_when(
  #   containsDisasterKeyword == T ~ "True",
  #   containsDisasterKeyword == F ~ "False",
  #   T ~ "Empty"
  # ))
tidy_valid <- checkDisasterKeyword(tidy_valid, disaster_keywords)
tidy_test <- checkDisasterKeyword(tidy_test, disaster_keywords)
tidy_trainset
```

## Ratio of each tags vs target

```{r}
tidy_trainset %>%
  filter(!is.na(tags)) %>%
  separate_rows(tags, sep = ',') %>%
  group_by(tags, target) %>%
  summarise(n = n()) %>%
  mutate(ratio = n / sum(n)) %>%
  ungroup() %>%
  arrange(desc(n), desc(ratio), tags)
```

## Ratio of each at vs target

```{r}
at_ratio <- tidy_trainset %>%
  filter(!is.na(at)) %>%
  separate_rows(at, sep = ',') %>%
  group_by(at, target) %>%
  summarise(n = n()) %>%
  mutate(ratio = n / sum(n)) %>%
  arrange(desc(n), desc(ratio), at)
at_ratio
```

## Create disaster at

```{r}
count_threshold <- 5
ratio_threshold <- 0.7
disaster_at <- at_ratio %>%
  filter(target == 1 & ratio >= ratio_threshold & n >= count_threshold) %>%
  .$at

checkDisasterAt <- function(dataset, disaster_at){
  dataset %>%
    mutate(
      contains_disaster_at = str_detect(
        at,
        paste(disaster_at, collapse = "|")
      )
    ) %>% 
    mutate(contains_disaster_at = case_when(
      contains_disaster_at == T ~ "True",
      contains_disaster_at == F ~ "False",
      T ~ "Empty"
    ))
}

tidy_trainset <- checkDisasterAt(tidy_trainset, disaster_at)
  # tidy_trainset %>%
  # mutate(
  #   containsDisasterAt = str_detect(
  #     at,
  #     paste(disasterAt, collapse = "|")
  #   )
  # ) %>% 
  # mutate(containsDisasterAt = case_when(
  #   containsDisasterAt == T ~ "True",
  #   containsDisasterAt == F ~ "False",
  #   T ~ "Empty"
  # ))
tidy_valid <- checkDisasterAt(tidy_valid, disaster_at)
tidy_test <- checkDisasterAt(tidy_test, disaster_at)
tidy_trainset
```

## Ratio of each word in text vs target

```{r}
word_ratio <- tidy_trainset %>%
  mutate(word = tolower(text)) %>%
  separate_rows(word) %>%
  mutate(word = str_extract(word, "[a-z]+")) %>%
  filter(is.na(word) == F & word != "") %>%
  anti_join(stop_words) %>%
  select(word, target) %>%
  group_by(word, target) %>%
  summarise(n = n()) %>%
  mutate(ratio = n / sum(n)) %>%
  ungroup() %>%
  arrange(desc(n), desc(ratio)) %>%
  filter(target == 1 & ratio >= ratio_threshold & n >= count_threshold)
word_ratio
```

## Create disaster words

```{r}
excluded_words <- c(
  "pm",
  "mh",
  "yr",
  "california",
  "hiroshima",
  "japan",
  "malaysia",
  "saudi",
  "boy",
  "island",
  "pkk"
)
count_threshold <- 25
ratio_threshold <- 0.65
disaster_words <- word_ratio %>%
  filter(target == 1 & ratio >= ratio_threshold & n >= count_threshold & !(word %in% excluded_words)) %>%
  .$word

checkDisasterWord <- function(dataset, disaster_words){
  dataset %>%
    mutate(
      contains_disaster_word = str_detect(
        text,
        paste(disaster_words, collapse = "|")
      )
    )
}

tidy_trainset <- checkDisasterWord(tidy_trainset, disaster_words)
  # tidy_trainset %>%
  # mutate(
  #   containsDisasterWord = str_detect(
  #     text,
  #     paste(disaster_words, collapse = "|")
  #   )
  # )
tidy_valid <- checkDisasterWord(tidy_valid, disaster_words)
tidy_test <- checkDisasterWord(tidy_test, disaster_words)

tidy_trainset
```

## Parse *location* variable (todo)

```{r}
withLocation <- tidy_trainset %>%
  mutate(location = str_replace_all(location, regex("[^, [:alpha:]]+"), "")) %>%
  mutate(location = str_replace_all(location, regex("( )*,( )*"), ", ")) %>%
  filter(location != '') %>%
  separate(location, c('location 1', 'location 2'), ', ') %>%
  filter(!(`location 1` %in% c('', NA)) | !(`location 2`%in% c('', NA)))
withLocation

withoutLocation <- tidy_trainset %>% anti_join(withLocation, by='id')
withoutLocation
USAstate <- cbind(state.abb, state.name) %>%
  as.tibble() %>%
  rename(abb = state.abb, name = state.name)
USAstate
```

```{r}
library(ISOcodes)
country_t <- ISO_3166_1 %>%
  as.tibble()
country_t
```

# Lexicon

```{r}
lexicon_nrc() %>%
  filter(word == 'earthquake')
lexicon_nrc_eil() %>%
  filter(term == 'fire')
```

# Random Forest Training

```{r}
# tidy_trainset
input <- tidy_trainset %>%
  # mutate(
  #   target = as.factor(target)
  #   # hasTag = case_when(
  #   #   is.na(tags) ~ 0,
  #   #   T ~ 1
  #   # ),
  #   # hasAt = case_when(
  #   #   is.na(at) ~ 0,
  #   #   T ~ 1
  #   # ),
  #   # hasLink = case_when(
  #   #   is.na(links) ~ 0,
  #   #   T ~ 1
  #   # )
  # ) %>%
  # select(target, hasTag, hasAt, charLengthType, wordCountType, keyword, containsDisasterWord)
  select(target, wordCountType, containsDisasterWord, containsDisasterKeyword, containsDisasterAt)

# for(i in seq(from = 10, to = 500, by = 10)){
#   rf <- randomForest(
#     target ~.,
#     data = input,
#     importance = T,
#     ntree = i,
#     mtry = 3
#   )
#   # print(rf$err.rate)
#   cat(paste0("ntree = ", i, "; OOB = ", rf$err.rate[i, 1]), "\n")
# }
# rf <- randomForest(target ~., data = input, importance = T, ntree = 300)
# rf
# 
valid_input <- tidy_valid %>%
  mutate(target = as.factor(target)) %>%
  select(target, wordCountType, containsDisasterWord, containsDisasterKeyword, containsDisasterAt)

test_input <- tidy_test %>%
  mutate(target = as.factor(target)) %>%
  select(target, wordCountType, containsDisasterWord, containsDisasterKeyword, containsDisasterAt)

p1 <- predict(rf, valid_input)
p1 %>%
  confusionMatrix(valid_input$target)
```

```{r}
tidy_trainset
input <- tidy_trainset %>%
  mutate(
    contains_disaster_at = as.numeric(factor(contains_disaster_at)),
    contains_disaster_word = as.numeric(factor(contains_disaster_word)),
    contains_disaster_keyword = as.numeric(factor(contains_disaster_keyword)),
    word_count_type = as.numeric(factor(word_count_type))
  ) %>%
  select(
    target,
    tag_count,
    at_count,
    link_count,
    char_length,
    word_count,
    mean_word_length,
    unique_word_count,
    punc_count,
    stop_word_count,
    contains_disaster_keyword,
    contains_disaster_at,
    contains_disaster_word
  )
input
```

```{r}
library(neuralnet)

network <- neuralnet(
    target ~
      tag_count +
      at_count +
      link_count +
      char_length +
      word_count +
      mean_word_length +
      unique_word_count +
      punc_count +
      stop_word_count
    ,
    data = input,
    rep = 2,
    hidden = 3,
    threshold = 0.1,
    linear.output = F,
    lifesign = "full",
    act.fct = 'logistic',
    err.fct = 'ce',
    # stepmax = 1000000,
    likelihood = T
  )
network$result.matrix
plot(network)
```

```{r}
network$result.matrix[1,]
plot(network, rep = "best")
```

```{r}
input <- tidy_valid %>%
  mutate(
    containsDisasterAt = as.numeric(factor(containsDisasterAt)),
    containsDisasterWord = as.numeric(factor(containsDisasterWord)),
    containsDisasterKeyword = as.numeric(factor(containsDisasterKeyword)),
    wordCountType = as.numeric(factor(wordCountType))
  ) %>%
  select(target, charLength, wordCount)
```

```{r}
network.results <- compute(network, input, rep = which.min(network$result.matrix[1, ]))
results <- data.frame(actual = input$target, prediction = network.results$net.result)
results
# pred.prob <- predict(network, input)
# pred.class <- ifelse(pred.prob > 0.5, 1, 0)
# pred.prob
pred.class <- sapply(results, round, digits=0) %>% as_tibble
pred.class
t <- table(pred.class$actual, input$target)
t
sum(diag(t))/sum(t)

```

# Twitter API

```{r}
twitter <- fromJSON("twitter.json")
twitterAPIHeader <- c('Authorization' = sprintf(""))
```
