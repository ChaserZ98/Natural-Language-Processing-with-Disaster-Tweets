---
title: "Final Project"
author: "Feiyu Zheng (fz114) & Junyi Li()"
date: "2022/3/27"
output:
  pdf_document: default
  html_document:
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(tidyverse)
library(lubridate)
library(ggplot2)
library(tidytext)
library(textdata)
library(rvest)
library(httr)
library(curl)
library(jsonlite)
library(randomForest)

# suppress the warning message of dplyr
options(dplyr.summarise.inform = F)
```

# Dataset

## Download source data from kaggle

```{r warning=FALSE}
kaggle <- fromJSON("kaggle.json")

if(!dir.exists("./Data")){
  dir.create("./Data")
}
if(!file.exists("./Data/train.csv")){
  data <- httr::GET("https://www.kaggle.com/api/v1/competitions/data/download/17777/train.csv", httr::authenticate(kaggle$username, kaggle$key, type = "basic"))
  download.file(data$url, "./Data/train.csv")
  data <- httr::GET("https://www.kaggle.com/api/v1/competitions/data/download/17777/test.csv", httr::authenticate(kaggle$username, kaggle$key, type = "basic"))
  download.file(data$url, "./Data/test.csv")
}
```

## Read downloaded csv files and convert them to tibble

```{r}
encoding <- "UTF-8"
trainset <- read.csv(
  "./Data/train.csv",
  encoding = encoding,
  header = T
) %>%
  as_tibble()

testset <- read.csv(
  "./Data/test.csv",
  encoding = encoding,
  header = T
) %>%
  as_tibble()


trainset
testset
```

## 

## Data Preprocess

### Simple analysis on the dataset

```{r}
trainsetRatio <- trainset %>%
  mutate(total = n()) %>%
  group_by(target, total) %>%
  summarise(count = n()) %>%
  mutate(ratio = round(count / total, 2)) %>%
  select(target, count, total, ratio) %>%
  ungroup()
trainsetRatio
```

### Dataset split according to target ratio

```{r}
total <- nrow(trainset)
zeroRatio <- trainsetRatio %>%
  filter(target == 0) %>%
  pull(ratio)
oneRatio <- trainsetRatio %>%
  filter(target == 1) %>%
  pull(ratio)

# set random seed for sample
set.seed(25)

# extract 20% data from trainset as validset
valRatio <- 0.2
zero <- trainset %>%
  filter(target == 0) %>%
  sample_n(zeroRatio * total * valRatio)
one <- trainset %>%
  filter(target == 1) %>%
  sample_n(oneRatio * total * valRatio)
valset <- bind_rows(
  sample_n(
    filter(trainset, target == 0),
    zeroRatio * total * valRatio
  ),
  sample_n(
    filter(trainset, target == 1),
    oneRatio * total * valRatio
  )
) %>%
  arrange(id)

# exclude those validset data from trainset
trainset <- trainset %>%
  anti_join(valset, by = "id") %>%
  arrange(id)

trainset
valset
```

### Replace the whitespace unicode %20 in *keywords* variable and newline character \n in text

```{r}
replace_space_unicode <- function(dataset){
  dataset %>% mutate(
    keyword = str_replace_all(keyword, "%20", " "),
    text = str_replace_all(text, "\n", " ")
  )
}
replace_new_line <- function(dataset){
  dataset %>% mutate(
    text = str_replace_all(text, "\n", " ")
  )
}
tidy_trainset <- trainset %>%
  replace_space_unicode() %>%
  replace_new_line()
tidy_trainset %>% filter(id == 2899)

tidy_trainset %>%
  group_by(keyword) %>%
  count() %>%
  arrange(desc(n)) %>%
  ungroup()
```

### Extract Tags, '\@'s and links in *text* variable

```{r}
extract_at <- function(dataset){
  dataset %>%
    mutate(
      at = map_chr(
        map(
          .x = str_extract_all(tolower(.$text), "@\\S+"),
          unlist
        ),
        ~case_when(
          paste(.x, collapse = ',') == '' ~ NA_character_,
          T ~ paste(.x, collapse = ',')
        )
      ),
      text = str_replace_all(text, "@\\S+", '')
    )
}
extract_links <- function(dataset){
  dataset %>%
    mutate(
      links = map_chr(
        map(
          .x = str_extract_all(text, '\\S+\\.\\S+/\\S\\s*'),
          unlist
        ),
        ~case_when(
          paste(.x, collapse = ',') == '' ~ NA_character_,
          T ~ paste(.x, collapse = ',')
        )
      ),
      text = str_replace_all(text, "\\S+//\\S+\\.?\\S+\\S\\s*", '')
    )
}
extract_tags <- function(dataset){
  dataset %>%
    mutate(
      tags = map_chr(
        map(
          .x = str_extract_all(tolower(.$text), "#\\S+"),
          unlist
        ),
        ~case_when(
          paste(.x, collapse = ',') == '' ~ NA_character_,
          T ~ paste(.x, collapse = ',')
        )
      )
    )
}
tidy_trainset <- tidy_trainset %>%
  extract_tags() %>%
  extract_at() %>%
  extract_links()
tidy_trainset
```

### Compute the length of each *text* variable

```{r}
extract_char_length <- function(dataset){
  dataset %>% mutate(charLength = nchar(.$text))
}
extract_word_count <- function(dataset){
  dataset %>% mutate(wordCount = str_count(text, "\\w+"))
}

classify_char_length <- function(dataset){
  dataset %>% mutate(
    charLengthType = case_when(
      charLength <= 50 ~ "short",
      charLength <= 100 ~ "medium",
      charLength <= 150 ~ "long",
      T ~ "other"
    )
  )
}

tidy_trainset <- tidy_trainset %>%
  extract_char_length() %>%
  extract_word_count()
tidy_trainset <- tidy_trainset %>%
  classify_char_length()
tidy_trainset %>%
  mutate(target = as.factor(target)) %>%
  ggplot() +
  geom_bar(aes(x = charLengthType, fill = target)) +
  labs(
    x = "Length Type",
    y = "Count",
    title = "Distribution of Length Types"
  ) +
  theme(plot.title = element_text(hjust = 0.5))

tidy_trainset %>%
  mutate(target = as.factor(target)) %>%
  ggplot() +
  geom_histogram(aes(x = wordCount, fill = as.factor(target)))

tidy_trainset
```

### Ratio of each keyword vs target

```{r}
keywordRatio <- tidy_trainset %>%
  group_by(keyword) %>%
  summarise(disasterRatio = sum(target) / n()) %>%
  arrange(desc(disasterRatio))
keywordRatio
```

```{r}
tidy_trainset %>%
  filter(!is.na(tags)) %>%
  separate_rows(tags, sep = ',') %>%
  group_by(tags, target) %>%
  summarise(n = n()) %>%
  # group_by(tags) %>%
  mutate(ratio = n / sum(n)) %>%
  filter(n >= 10) %>%
  arrange(desc(n), desc(ratio), tags)
```

### Parse *location* variable (todo)

```{r}
withLocation <- tidy_trainset %>%
  mutate(location = str_replace_all(location, regex("[^, [:alpha:]]+"), "")) %>%
  mutate(location = str_replace_all(location, regex("( )*,( )*"), ", ")) %>%
  filter(location != '') %>%
  separate(location, c('location 1', 'location 2'), ', ') %>%
  filter(!(`location 1` %in% c('', NA)) | !(`location 2`%in% c('', NA)))
withLocation

withoutLocation <- tidy_trainset %>% anti_join(withLocation, by='id')
withoutLocation
USAstate <- cbind(state.abb, state.name) %>%
  as.tibble() %>%
  rename(abb = state.abb, name = state.name)
USAstate
```

```{r}
library(ISOcodes)
country_t <- ISO_3166_1 %>%
  as.tibble()
country_t
```

## Lexicon

```{r}
lexicon_nrc() %>%
  filter(word == 'earthquake')
lexicon_nrc_eil() %>%
  filter(term == 'fire')
```

## Random Forest Training

```{r}
# tidy_trainset
input <- tidy_trainset %>%
  mutate(
    target = as.factor(target),
    hasTag = case_when(
      tags == '' ~ 0,
      T ~ 1
    ),
    hasAt = case_when(
      at == '' ~ 0,
      T ~ 1
    )
  ) %>%
  select(target, hasTag, hasAt)
randomForest(target ~., data = input, importance = T)
```

## Twitter API

```{r}
twitter <- fromJSON("twitter.json")
twitterAPIHeader <- c('Authorization' = sprintf(""))
```
