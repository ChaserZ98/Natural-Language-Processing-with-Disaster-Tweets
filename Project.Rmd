---
title: "Final Project"
author: "Feiyu Zheng (fz114) & Junyi Li() & Ziyu Zhou ()"
date: "2022/3/27"
output:
  pdf_document: default
  html_document:
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(tidyverse)
library(lubridate)
library(ggplot2)
library(tidytext)
library(textdata)
library(rvest)
library(httr)
library(curl)
library(jsonlite)
library(caret)
library(gridExtra)
library(scales)

# suppress the warning message of dplyr
options(dplyr.summarise.inform = F)
```

# Obtain Data

## Download source data from kaggle

```{r warning=FALSE}
kaggle <- fromJSON("./Config/kaggle.json")

if(!dir.exists("./Data")){
  dir.create("./Data")
}
if(!file.exists("./Data/train.csv")){
  data <- httr::GET(
    "https://www.kaggle.com/api/v1/competitions/data/download/17777/train.csv",
    httr::authenticate(
      kaggle$username,
      kaggle$key,
      type = "basic"
    )
  )
  download.file(data$url, "./Data/train.csv")
  data <- httr::GET(
    "https://www.kaggle.com/api/v1/competitions/data/download/17777/test.csv",
    httr::authenticate(
      kaggle$username,
      kaggle$key,
      type = "basic"
    )
  )
  download.file(data$url, "./Data/test.csv")
  download.file(
    "https://raw.githubusercontent.com/shahp7575/disaster-tweets/master/socialmedia-disaster-tweets-DFE.csv",
    "./Data/socialmedia-disaster-tweets-DFE.csv"
  )
}
```

## Read downloaded csv files and convert them to tibble

```{r}
encoding <- "UTF-8"
trainset <- read.csv(
  "./Data/train.csv",
  encoding = encoding,
  header = T
) %>%
  as_tibble()

testset <- read.csv(
  "./Data/test.csv",
  encoding = encoding,
  header = T
) %>%
  as_tibble()

# add target value to testset
test_with_target <- read.csv(
  "./Data/socialmedia-disaster-tweets-DFE.csv",
  encoding = encoding,
  header = T
) %>%
  as_tibble()

test_with_target %>%
  mutate(id = row_number() - 1) %>%
  select(id, text, choose_one) %>%
  inner_join(testset, by = 'id') %>%
  select(id, keyword, location, text.y, choose_one) %>%
  rename(text = text.y, target = choose_one) %>%
  mutate(target = case_when(
    target == "Relevant" ~ as.integer(1),
    T ~ as.integer(0)
  )) %>%
  write.csv(file = "./Data/test.csv", row.names = F)

testset <- read.csv(
  "./Data/test.csv",
  encoding = encoding,
  header = T
) %>%
  as_tibble()

trainset
testset
```

# Data Preprocess

## Simple analysis on the dataset

### Missing Values

```{r}
computeMissingValue <- function(dataset){
  rbind(
    dataset %>%
      mutate(isMissing = keyword == '' | is.na(keyword)) %>%
      group_by(isMissing) %>%
      count() %>%
      ungroup() %>%
      mutate(valueType = "keyword"),
    dataset %>%
      mutate(isMissing = location == '' | is.na(location)) %>%
      group_by(isMissing) %>%
      count() %>%
      ungroup() %>%
      mutate(valueType = "location")
  )
}

missingValues_train <- computeMissingValue(trainset)
missingValues_test <- computeMissingValue(testset)

missingValuePlot_train <- missingValues_train %>%
  ggplot(aes(x = valueType, y = n, fill = isMissing)) +
  geom_bar(position = "stack", stat = "identity") +
  labs(
    x = "Variable Name",
    y = "Count",
    title = "Training Set Missing Value Count"
  ) +
  theme(plot.title = element_text(hjust = 0.5))
missingValuePlot_test <- missingValues_test %>%
  ggplot(aes(x = valueType, y = n, fill = isMissing)) +
  geom_bar(position = "stack", stat = "identity") +
  labs(
    x = "Variable Name",
    y = "Count",
    title = "Testing Set Missing Value Count"
  ) +
  theme(plot.title = element_text(hjust = 0.5))

grid.arrange(missingValuePlot_train, missingValuePlot_test, nrow = 1)
```

### Target Distribution

```{r}
trainsetRatio <- trainset %>%
  mutate(total = n()) %>%
  group_by(target, total) %>%
  summarise(count = n()) %>%
  mutate(ratio = round(count / total, 2)) %>%
  select(target, count, total, ratio) %>%
  ungroup()
pie_chart <- trainsetRatio %>%
  mutate(
    target = case_when(
      target == 1 ~ "Disaster",
      T ~ "Not Disaster"
    )
  ) %>%
  ggplot(aes(x = '', y = count, fill = target)) +
  geom_bar(stat = "identity") +
  geom_text(
    aes(
      y = count / 2 + c(0, cumsum(count)[-length(count)]),
      label = paste(percent(count / total), "(", count, ")", sep = '')
    ),
    size = 5
  ) +
  coord_polar("y", start = 0) +
  theme_minimal() +
  labs(
    title = "Target Distribution in Training Set",
    fill = "Target"
  ) +
  theme(
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    panel.border = element_blank(),
    panel.grid = element_blank(),
    axis.ticks = element_blank(),
    plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
    axis.text.x = element_blank()
  )

grid.arrange(pie_chart, nrow = 1)
```

## Train-validation split according to target ratio

```{r}
total <- nrow(trainset)
zeroRatio <- trainsetRatio %>%
  filter(target == 0) %>%
  pull(ratio)
oneRatio <- trainsetRatio %>%
  filter(target == 1) %>%
  pull(ratio)

# set random seed for sample
set.seed(25)

# extract 20% data from trainset as validset
valRatio <- 0.2
zero <- trainset %>%
  filter(target == 0) %>%
  sample_n(zeroRatio * total * valRatio)
one <- trainset %>%
  filter(target == 1) %>%
  sample_n(oneRatio * total * valRatio)
valset <- bind_rows(
  sample_n(
    filter(trainset, target == 0),
    zeroRatio * total * valRatio
  ),
  sample_n(
    filter(trainset, target == 1),
    oneRatio * total * valRatio
  )
) %>%
  arrange(id)

# exclude those validset data from trainset
trainset <- trainset %>%
  anti_join(valset, by = "id") %>%
  arrange(id)

trainset
valset
```

## Data Cleaning

### Replace the white space Unicode *%20* in *keywords* variable and newline character \n in text

```{r}
replace_space_unicode <- function(dataset){
  dataset %>%
    mutate(
      keyword = str_replace_all(keyword, "%20", " "),
      text = str_replace_all(text, "\n", " ")
    ) %>%
    mutate(
      keyword = na_if(keyword, "")
    )
}
replace_new_line <- function(dataset){
  dataset %>% mutate(
    text = str_replace_all(text, "\n", " ")
  )
}
tidy_trainset <- trainset %>%
  replace_space_unicode() %>%
  replace_new_line()
tidy_trainset

tidy_valid <- valset %>%
  replace_space_unicode() %>%
  replace_new_line()

tidy_test <- testset %>%
  replace_space_unicode() %>%
  replace_new_line()
```

### Extract Tags, '\@'s and links in *text* variable

```{r}
extract_at <- function(dataset){
  dataset %>%
    mutate(
      at = str_replace_all(
        map_chr(
          map(
            .x = str_extract_all(tolower(.$text), "@\\S+"),
            unlist
          ),
          ~case_when(
            paste(.x, collapse = ',') == '' ~ NA_character_,
            T ~ paste(.x, collapse = ',')
          )
        ),
        '@',
        ''
      ),
      at_count = str_count(tolower(.$text), "@\\S+"),
      text = str_replace_all(text, "@\\S+", '')
    )
}
extract_links <- function(dataset){
  dataset %>%
    mutate(
      links = map_chr(
        map(
          .x = str_extract_all(tolower(.$text), '\\S+\\.\\S+/\\S\\s*'),
          unlist
        ),
        ~case_when(
          paste(.x, collapse = ',') == '' ~ NA_character_,
          T ~ paste(.x, collapse = ',')
        )
      ),
      link_count = str_count(tolower(.$text), '\\S+\\.\\S+/\\S\\s*'),
      text = str_replace_all(text, "\\S+//\\S+\\.?\\S+\\S\\s*", '')
    )
}
extract_tags <- function(dataset){
  dataset %>%
    mutate(
      tags = str_replace_all(
        map_chr(
          map(
            .x = str_extract_all(tolower(.$text), "#\\S+"),
            unlist
          ),
          ~case_when(
            paste(.x, collapse = ',') == '' ~ NA_character_,
            T ~ paste(.x, collapse = ',')
          )
        ),
        '#',
        ''
      ),
      tag_count = str_count(tolower(.$text), "#\\S+"),
      text = str_replace_all(text, "#", '')
    )
}
tidy_trainset <- tidy_trainset %>%
  extract_tags() %>%
  extract_at() %>%
  extract_links()
tidy_trainset

tidy_valid <- tidy_valid %>%
  extract_tags() %>%
  extract_at() %>%
  extract_links()

tidy_test <- tidy_test %>%
  extract_tags() %>%
  extract_at() %>%
  extract_links()
```

### (todo) Parse *location* variable

```{r}
library(ISOcodes)
country_t <- ISO_3166_1 %>%
  as_tibble()
country_t
```

```{r}
withLocation <- tidy_trainset %>%
  mutate(location = str_replace_all(location, regex("[^, [:alpha:]]+"), "")) %>%
  mutate(location = str_replace_all(location, regex("( )*,( )*"), ", ")) %>%
  filter(location != '') %>%
  separate(location, c('location 1', 'location 2'), ', ') %>%
  filter(!(`location 1` %in% c('', NA)) | !(`location 2`%in% c('', NA)))
withLocation

withoutLocation <- tidy_trainset %>% anti_join(withLocation, by='id')
withoutLocation
USAstate <- cbind(state.abb, state.name) %>%
  as_tibble() %>%
  rename(abb = state.abb, name = state.name)
USAstate
```

## Feature Engineering

### Extract *char length*, *word count*, *mean word count*, *unique word count*, *punctuation count*, *stop words count*

```{r}
extract_char_length <- function(dataset){
  dataset %>%
    mutate(char_length = nchar(.$text))
}
extract_word_count <- function(dataset){
  dataset %>%
    mutate(word_count = str_count(text, "\\w+"))
}
extract_mean_word_length <- function(dataset){
  dataset %>%
    mutate(mean_word_length = char_length/word_count)
}
extract_unique_word_count <- function(dataset){
  dataset %>%
    mutate(
      unique_word_count = map_int(
        .$text,
        ~ .x %>% str_extract_all("\\w+") %>% unlist %>% n_distinct
      )
    )
}
extract_punctuation_count <- function(dataset){
  dataset %>%
    mutate(
      punc_count = str_count(text, "[[:punct:]]+")
    )
}
extract_stop_word_count <- function(dataset){
  dataset %>%
  mutate(
    stop_word_count = map_int(
      text,
      ~ .x %>%
        tolower() %>%
        str_split(' ') %>%
        map(., ~case_when(. %in% stop_words$word ~ T, T ~ F)) %>%
        unlist %>%
        sum()
    )
  )
}

classify_char_length <- function(dataset){
  dataset %>%
    mutate(
      char_length_type = case_when(
        char_length <= 50 ~ "short",
        char_length <= 100 ~ "medium",
        char_length <= 150 ~ "long",
        T ~ "other"
      )
    )
}
classify_word_length <- function(dataset){
  dataset %>%
    mutate(
      word_count_type = case_when(
        word_count <= 5 ~ "0-5",
        word_count <= 10 ~ "5-10",
        word_count <= 15 ~ "10-15",
        word_count <= 20 ~ "15-20",
        word_count <= 25 ~ "20-25",
        word_count <= 30 ~ "25-30",
        T ~ "other"
      )
    )
}

tidy_trainset <- tidy_trainset %>%
  extract_char_length() %>%
  extract_word_count() %>%
  extract_mean_word_length() %>%
  extract_unique_word_count() %>%
  extract_punctuation_count() %>%
  extract_stop_word_count()

tidy_valid <- tidy_valid %>%
  extract_char_length() %>%
  extract_word_count() %>%
  extract_mean_word_length() %>%
  extract_unique_word_count() %>%
  extract_punctuation_count() %>%
  extract_stop_word_count()

tidy_test <- tidy_test %>%
  extract_char_length() %>%
  extract_word_count() %>%
  extract_mean_word_length() %>%
  extract_unique_word_count() %>%
  extract_punctuation_count() %>%
  extract_stop_word_count()

tidy_trainset <- tidy_trainset %>%
  classify_char_length() %>%
  classify_word_length()

tidy_valid <- tidy_valid %>%
  classify_char_length() %>%
  classify_word_length()

tidy_test <- tidy_test %>%
  classify_char_length() %>%
  classify_word_length()

tidy_trainset %>%
  mutate(target = as.factor(target)) %>%
  ggplot() +
  geom_bar(aes(x = char_length_type, fill = target)) +
  labs(
    x = "Length Type",
    y = "Count",
    title = "Distribution of Length Types"
  ) +
  theme(plot.title = element_text(hjust = 0.5))

tidy_trainset %>%
  mutate(target = as.factor(target)) %>%
  ggplot() +
  geom_histogram(aes(x = word_count, fill = as.factor(target))) +
  labs(
    x = "word count",
    y = "Count",
    title = "Distribution of word count"
  ) +
  theme(plot.title = element_text(hjust = 0.5))

tidy_trainset %>%
  mutate(target = as.factor(target)) %>%
  ggplot() +
  geom_bar(aes(x = word_count_type, fill = target)) +
  labs(
    x = "Word Count Type",
    y = "Count",
    title = "Distribution of Length Types"
  ) +
  theme(plot.title = element_text(hjust = 0.5))

tidy_trainset
```

### Keyword

#### Ratio of each keyword vs target

```{r}
keyword_ratio <- tidy_trainset %>%
  group_by(keyword, target) %>%
  summarise(n = n()) %>%
  mutate(ratio = n / sum(n)) %>%
  ungroup() %>%
  arrange(desc(n), desc(ratio))
keyword_ratio
```

#### Create disaster keyword

```{r}
count_threshold <- 20
ratio_threshold <- 0.8
disaster_keywords <- keyword_ratio %>%
  filter(n >= count_threshold & target == 1 & ratio >= ratio_threshold) %>%
  .$keyword

checkDisasterKeyword <- function(dataset, disaster_keywords){
  dataset %>%
    mutate(
      contains_disaster_keyword = str_detect(
        keyword,
        paste(disaster_keywords, collapse = "|")
      )
    ) %>%
    mutate(contains_disaster_keyword = case_when(
      contains_disaster_keyword == T ~ "True",
      contains_disaster_keyword == F ~ "False",
      T ~ "Empty"
    ))
}

tidy_trainset <- checkDisasterKeyword(tidy_trainset, disaster_keywords)
  # tidy_trainset %>%
  # mutate(
  #   containsDisasterKeyword = str_detect(
  #     keyword,
  #     paste(disasterKeywords, collapse = "|")
  #   )
  # ) %>%
  # mutate(containsDisasterKeyword = case_when(
  #   containsDisasterKeyword == T ~ "True",
  #   containsDisasterKeyword == F ~ "False",
  #   T ~ "Empty"
  # ))
tidy_valid <- checkDisasterKeyword(tidy_valid, disaster_keywords)
tidy_test <- checkDisasterKeyword(tidy_test, disaster_keywords)
tidy_trainset
```

### Tag

#### Ratio of each tags vs target

```{r}
tag_ratio <- tidy_trainset %>%
  filter(!is.na(tags)) %>%
  separate_rows(tags, sep = ',') %>%
  group_by(tags, target) %>%
  summarise(n = n()) %>%
  mutate(ratio = n / sum(n)) %>%
  ungroup() %>%
  arrange(desc(n), desc(ratio), tags)
tag_ratio
```

```{r}
str <- "tags"
str_detect(str, str_replace_all("^(???|tag)$", '\\?', '\\\\?'))
```

#### Create disaster tag

```{r}
tidy_trainset %>%
  select(tags, contains_disaster_tag)
disaster_tag
```

```{r}
count_threshold <- 5
ratio_threshold <- 0.7
disaster_tag <- tag_ratio %>%
  filter(target == 1 & ratio >= ratio_threshold & n >= count_threshold) %>%
  .$tags

checkDisasterTag <- function(dataset, disaster_tag){
  dataset %>%
    mutate(
      contains_disaster_tag = str_detect(
        tags,
        paste(
          "^(",
          str_replace_all(
            paste(disaster_tag, collapse = "|"),
            '\\?',
            '\\\\?'
          ),
          ")$",
          sep = ""
        )
      )
    ) %>% 
    mutate(contains_disaster_tag = case_when(
      contains_disaster_tag == T ~ "True",
      contains_disaster_tag == F ~ "False",
      T ~ "Empty"
    ))
}
tidy_trainset <- checkDisasterTag(tidy_trainset, disaster_tag)
tidy_valid <- checkDisasterTag(tidy_valid, disaster_tag)
tidy_test <- checkDisasterTag(tidy_test, disaster_tag)
tidy_trainset
```

### At

#### Ratio of each at vs target

```{r}
at_ratio <- tidy_trainset %>%
  filter(!is.na(at)) %>%
  separate_rows(at, sep = ',') %>%
  group_by(at, target) %>%
  summarise(n = n()) %>%
  mutate(ratio = n / sum(n)) %>%
  arrange(desc(n), desc(ratio), at)
at_ratio
```

#### Create disaster at

```{r}
count_threshold <- 5
ratio_threshold <- 0.7
disaster_at <- at_ratio %>%
  filter(target == 1 & ratio >= ratio_threshold & n >= count_threshold) %>%
  .$at

checkDisasterAt <- function(dataset, disaster_at){
  dataset %>%
    mutate(
      contains_disaster_at = str_detect(
        at,
        paste(disaster_at, collapse = "|")
      )
    ) %>% 
    mutate(contains_disaster_at = case_when(
      contains_disaster_at == T ~ "True",
      contains_disaster_at == F ~ "False",
      T ~ "Empty"
    ))
}

tidy_trainset <- checkDisasterAt(tidy_trainset, disaster_at)
  # tidy_trainset %>%
  # mutate(
  #   containsDisasterAt = str_detect(
  #     at,
  #     paste(disasterAt, collapse = "|")
  #   )
  # ) %>% 
  # mutate(containsDisasterAt = case_when(
  #   containsDisasterAt == T ~ "True",
  #   containsDisasterAt == F ~ "False",
  #   T ~ "Empty"
  # ))
tidy_valid <- checkDisasterAt(tidy_valid, disaster_at)
tidy_test <- checkDisasterAt(tidy_test, disaster_at)
tidy_trainset
```

### Text

#### Ratio of each word in text vs target

```{r}
word_ratio <- tidy_trainset %>%
  mutate(word = tolower(text)) %>%
  separate_rows(word) %>%
  mutate(word = str_extract(word, "[a-z]+")) %>%
  filter(is.na(word) == F & word != "") %>%
  anti_join(stop_words) %>%
  select(word, target) %>%
  group_by(word, target) %>%
  summarise(n = n()) %>%
  mutate(ratio = n / sum(n)) %>%
  ungroup() %>%
  arrange(desc(n), desc(ratio)) %>%
  filter(target == 1 & ratio >= ratio_threshold & n >= count_threshold)
word_ratio
```

#### Create disaster words

```{r}
str <- "Our Deeds are the Reason of this earthquake May ALLAH Forgive us all" %>%
  tolower()
str_detect(
  str,
  paste(
    "^(",
    paste(disaster_words, collapse = "|"),
    ")$",
    sep = ""
  )
)
```

```{r}
# excluded_words <- c(
#   "pm",
#   "mh",
#   "yr",
#   "california",
#   "hiroshima",
#   "japan",
#   "malaysia",
#   "saudi",
#   "boy",
#   "island",
#   "pkk"
# )
count_threshold <- 25
ratio_threshold <- 0.65
disaster_words <- word_ratio %>%
  filter(target == 1 & ratio >= ratio_threshold & n >= count_threshold) %>%
  .$word

checkDisasterWord <- function(dataset, disaster_words){
  dataset %>%
    mutate(
      contains_disaster_word = str_detect(
        text,
        paste(
          # "^(",
          paste(disaster_words, collapse = "|"),
          # ")$",
          sep = ""
        )
      )
    )
}

tidy_trainset <- checkDisasterWord(tidy_trainset, disaster_words)
  # tidy_trainset %>%
  # mutate(
  #   containsDisasterWord = str_detect(
  #     text,
  #     paste(disaster_words, collapse = "|")
  #   )
  # )
tidy_valid <- checkDisasterWord(tidy_valid, disaster_words)
tidy_test <- checkDisasterWord(tidy_test, disaster_words)

tidy_trainset
```

#### N-gram analysis

##### Unigram

```{r}
unigrams <- tidy_trainset %>%
  unnest_tokens(unigram, text, token = "ngrams", n = 1) %>%
  filter(unigram != "NA") %>%
  filter(str_detect(unigram, "^[a-z]+$")) %>%
  filter(!(unigram %in% stop_words$word)) %>%
  group_by(unigram, target) %>%
  count() %>%
  ungroup(target) %>%
  mutate(count = sum(n)) %>%
  mutate(ratio = n/count) %>%
  arrange(desc(n), desc(ratio)) %>%
  ungroup()
unigrams
```

```{r}
count_threshold <- 25
ratio_threshold <- 0.65
disasterUnigrams <- unigrams %>%
  filter(target == 1 & n >= count_threshold & ratio >= ratio_threshold) %>%
  arrange(desc(n)) %>%
  select(unigram, n)

nonDisasterUnigrams <- unigrams %>%
  filter(target == 0 & n >= count_threshold & ratio >= ratio_threshold) %>%
  arrange(desc(n)) %>%
  select(unigram, n)

top20DisasterUnigrams_plot <- disasterUnigrams %>%
  head(20) %>%
  ggplot(aes(reorder(unigram, n), n)) +
  geom_bar(stat = "identity") + 
  coord_flip() +
  labs(
    x = "Unigram",
    y = "Count",
    title = "Top 20 Disaster Unigrams"
  ) + 
  theme(plot.title = element_text(hjust = 0.5))

top20NonDisasterUnigrams_plot <- nonDisasterUnigrams %>%
  head(20) %>%
  ggplot(aes(reorder(unigram, n), n)) +
  geom_bar(stat = "identity") + 
  coord_flip() +
  labs(
    x = "Unigram",
    y = "Count",
    title = "Top 20 Non-Disaster Unigrams"
  ) + 
  theme(plot.title = element_text(hjust = 0.5))

grid.arrange(top20DisasterUnigrams_plot, top20NonDisasterUnigrams_plot, nrow = 1)
```

```{r}
checkDisasterUnigram <- function(dataset){
  
}
```

##### Bigram

```{r}
bigrams <- tidy_trainset %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
  filter(bigram != "NA") %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>%
  filter(str_detect(word1, "^[a-z]+$")) %>%
  filter(str_detect(word2, "^[a-z]+$")) %>%
  filter(!(word1 %in% stop_words$word)) %>%
  filter(!(word2 %in% stop_words$word)) %>%
  unite(bigram, word1, word2, sep = " ") %>%
  group_by(bigram, target) %>%
  count() %>%
  ungroup(target) %>%
  mutate(count = sum(n)) %>%
  ungroup() %>%
  mutate(ratio = n/count) %>%
  arrange(desc(count), desc(ratio))
bigrams
```

```{r}
count_threshold <- 10
ratio_threshold <- 0.65
disasterBigrams <- bigrams %>%
  filter(target == 1 & n >= count_threshold & ratio >= ratio_threshold) %>%
  arrange(desc(n)) %>%
  select(bigram, n)

top20DisasterBigrams_plot <- disasterBigrams %>%
  head(20) %>%
  ggplot(aes(reorder(bigram, n), n)) +
  geom_bar(stat = "identity") + 
  coord_flip() +
  labs(
    x = "Bigram",
    y = "Count",
    title = "Top 20 Disaster Bigrams"
  ) + 
  theme(plot.title = element_text(hjust = 0.5))

nonDisasterBigrams <- bigrams %>%
  filter(target == 0 & n >= count_threshold & ratio >= ratio_threshold) %>%
  arrange(desc(n)) %>%
  select(bigram, n)

top20NonDisasterBigrams_plot <- nonDisasterBigrams %>%
  head(20) %>%
  ggplot(aes(reorder(bigram, n), n)) +
  geom_bar(stat = "identity") + 
  coord_flip() +
  labs(
    x = "Bigram",
    y = "Count",
    title = "Top 20 Non-Disaster Bigrams"
  ) + 
  theme(plot.title = element_text(hjust = 0.5))

grid.arrange(top20DisasterBigrams_plot, top20NonDisasterBigrams_plot, nrow = 1)
```

##### Trigram

```{r}
trigrams <- tidy_trainset %>%
  unnest_tokens(trigram, text, token = "ngrams", n = 3) %>%
  filter(trigram != "NA") %>%
  separate(trigram, c("word1", "word2", "word3"), sep = " ") %>%
  filter(str_detect(word1, "^[a-z]+$")) %>%
  filter(str_detect(word2, "^[a-z]+$")) %>%
  filter(str_detect(word3, "^[a-z]+$")) %>%
  filter(!(word1 %in% stop_words$word)) %>%
  filter(!(word2 %in% stop_words$word)) %>%
  filter(!(word3 %in% stop_words$word)) %>%
  unite(trigram, word1, word2, word3, sep = " ") %>%
  group_by(trigram, target) %>%
  count() %>%
  ungroup(target) %>%
  mutate(count = sum(n)) %>%
  mutate(ratio = n/count) %>%
  arrange(desc(count), desc(ratio))
trigrams
```

```{r}
count_threshold <- 10
ratio_threshold <- 0.65
disasterTrigrams <- trigrams %>%
  filter(target == 1 & n >= count_threshold & ratio >= ratio_threshold) %>%
  arrange(desc(n)) %>%
  select(trigram, n)

top20DisasterTrigrams_plot <- disasterTrigrams %>%
  ggplot(aes(reorder(trigram, n), n)) +
  geom_bar(stat = "identity") + 
  coord_flip() +
  labs(
    x = "Trigram",
    y = "Count",
    title = "Top 20 Disaster Trigrams"
  ) + 
  theme(plot.title = element_text(hjust = 0.5))

nonDisasterTrigrams <- trigrams %>%
  filter(target == 0 & n >= count_threshold & ratio >= ratio_threshold) %>%
  arrange(desc(n)) %>%
  select(trigram, n)

top20NonDisasterTrigrams_plot <- nonDisasterTrigrams %>%
  ggplot(aes(reorder(trigram, n), n)) +
  geom_bar(stat = "identity") + 
  coord_flip() +
  labs(
    x = "Trigram",
    y = "Count",
    title = "Top 20 Non-Disaster Trigrams"
  ) + 
  theme(plot.title = element_text(hjust = 0.5))

grid.arrange(top20DisasterTrigrams_plot, top20NonDisasterTrigrams_plot, nrow = 1)
```

### Sentimental Analysis

#### Lexicon

```{r}
lexicon_nrc() %>%
  filter(word == 'earthquake')
lexicon_nrc_eil() %>%
  filter(term == 'fire')
```

# Model

## Random Forest Training

### Boosting

```{r}
library(gbm)
```

```{r}
tidy_trainset
```

```{r}
processModelInput <- function(dataset){
  dataset %>%
    mutate(
    contains_disaster_at = as.numeric(factor(contains_disaster_at)),
    contains_disaster_word = as.numeric(factor(contains_disaster_word)),
    contains_disaster_keyword = as.numeric(factor(contains_disaster_keyword)),
    contains_disaster_tag = as.numeric(factor(contains_disaster_tag)),
    word_count_type = as.numeric(factor(word_count_type))
  ) %>%
  select(
    target,
    tag_count,
    at_count,
    link_count,
    char_length,
    word_count,
    mean_word_length,
    unique_word_count,
    punc_count,
    stop_word_count,
    contains_disaster_keyword,
    contains_disaster_at,
    contains_disaster_word,
    contains_disaster_tag
  )
}
input_train <- tidy_trainset %>%
  processModelInput()
input_train

input_valid <- tidy_valid %>%
  processModelInput()
input_test <- tidy_test %>%
  processModelInput()
```

```{r}
boosting <- gbm(
  as.character(target) ~.,
  data = input_train,,
  n.trees = 5000,
  distribution = "adaboost",
  interaction.depth = 3,
  shrinkage = 0.2
)
boosting
```

```{r}
predict.result <- predict(boosting, input_valid, n.trees = 325, type = 'response')
predict.result[predict.result > 0.5] = 1
predict.result[predict.result <= 0.5] = 0
predict.result %>%
  as.factor() %>%
  confusionMatrix(as.factor(input_valid$target))
```

### Bagging

```{r}
library(randomForest)
```

```{r}
bagging <- randomForest(
  as.factor(target) ~.,
  data = input_train,
  importance = T,
  ntree = 2000
)
bagging
```

```{r}
p1 <- predict(bagging, input_valid)
p1 %>%
  as.factor() %>%
  confusionMatrix(as.factor(input_valid$target))
```

## Neural Network Training

```{r}
library(neuralnet)
set.seed(25)
network <- neuralnet(
    target ~
      tag_count +
      at_count +
      link_count +
      char_length +
      word_count +
      mean_word_length +
      unique_word_count +
      punc_count +
      stop_word_count +
      contains_disaster_keyword +
      contains_disaster_at +
      contains_disaster_word +
      contains_disaster_tag
    ,
    data = input_train,
    rep = 1,
    hidden = 5,
    threshold = 0.03,
    linear.output = F,
    lifesign = "full",
    lifesign.step = 5000,
    act.fct = 'logistic',
    err.fct = 'ce',
    stepmax = 1000000,
    likelihood = T
  )
network$result.matrix[1,]
plot(network, rep = "best")
```

```{r}
network.results <- compute(network, input_valid, rep = which.min(network$result.matrix[1, ]))
results <- data.frame(actual = input_valid$target, prediction = round(network.results$net.result))
results
t <- table(results$actual, results$prediction)
t
prop.table(t)
sum(diag(t))/sum(t)
```

# Twitter API

```{r}
twitter <- fromJSON("./Config/twitter.json")
twitterAPIHeader <- c('Authorization' = sprintf(""))
```
