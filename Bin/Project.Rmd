---
title: "Final Project"
author: "Feiyu Zheng (fz114) & Junyi Li(jl2707) & Ziyu Zhou (zz544)"
date: "2022/3/27"
output:
  pdf_document: default
  html_document:
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(tidyverse)
library(lubridate)
library(ggplot2)
library(tidytext)
library(textdata)
library(rvest)
library(httr)
library(curl)
library(jsonlite)
library(caret)
library(gridExtra)
library(scales)

# suppress the warning message of dplyr
options(dplyr.summarise.inform = F)
```

# Obtain Data

## Download source data from kaggle

```{r warning=FALSE}
kaggle <- fromJSON("../Config/kaggle.json")

if(!dir.exists("../Data")){
  dir.create("../Data")
}
if(!dir.exists("../Data/Kaggle")){
  dir.create("../Data/Kaggle")
}
if(!file.exists("../Data/Kaggle/train.csv")){
  data <- httr::GET(
    "https://www.kaggle.com/api/v1/competitions/data/download/17777/train.csv",
    httr::authenticate(
      kaggle$username,
      kaggle$key,
      type = "basic"
    )
  )
  download.file(data$url, "../Data/Kaggle/train.csv")
  data <- httr::GET(
    "https://www.kaggle.com/api/v1/competitions/data/download/17777/test.csv",
    httr::authenticate(
      kaggle$username,
      kaggle$key,
      type = "basic"
    )
  )
  download.file(data$url, "../Data/Kaggle/test.csv")
  download.file(
    "https://raw.githubusercontent.com/shahp7575/disaster-tweets/master/socialmedia-disaster-tweets-DFE.csv",
    "../Data/Kaggle/socialmedia-disaster-tweets-DFE.csv"
  )
}
```

## Read downloaded csv files and convert them to tibble

```{r}
encoding <- "UTF-8"
trainset <- read.csv(
  "../Data/Kaggle/train.csv",
  encoding = encoding,
  header = T
) %>%
  as_tibble()

testset <- read.csv(
  "../Data/Kaggle/test.csv",
  encoding = encoding,
  header = T
) %>%
  as_tibble()

# add target value to testset
test_with_target <- read.csv(
  "../Data/Kaggle/socialmedia-disaster-tweets-DFE.csv",
  encoding = encoding,
  header = T
) %>%
  as_tibble()

test_with_target %>%
  mutate(id = row_number() - 1) %>%
  select(id, text, choose_one) %>%
  inner_join(testset, by = 'id') %>%
  select(id, keyword, location, text.y, choose_one) %>%
  rename(text = text.y, target = choose_one) %>%
  mutate(target = case_when(
    target == "Relevant" ~ as.integer(1),
    T ~ as.integer(0)
  )) %>%
  write.csv(file = "../Data/Kaggle/test.csv", row.names = F)

testset <- read.csv(
  "../Data/Kaggle/test.csv",
  encoding = encoding,
  header = T
) %>%
  as_tibble()

trainset
testset
```

# Data Preprocess

## Simple analysis on the dataset

### Missing Values

```{r}
computeMissingValue <- function(dataset){
  rbind(
    dataset %>%
      mutate(isMissing = keyword == '' | is.na(keyword)) %>%
      group_by(isMissing) %>%
      count() %>%
      ungroup() %>%
      mutate(valueType = "keyword"),
    dataset %>%
      mutate(isMissing = location == '' | is.na(location)) %>%
      group_by(isMissing) %>%
      count() %>%
      ungroup() %>%
      mutate(valueType = "location")
  )
}

missingValues_train <- computeMissingValue(trainset)
missingValues_test <- computeMissingValue(testset)

missingValuePlot_train <- missingValues_train %>%
  ggplot(aes(x = valueType, y = n, fill = isMissing)) +
  geom_bar(position = "stack", stat = "identity") +
  labs(
    x = "Variable Name",
    y = "Count",
    title = "Training Set Missing Value Count"
  ) +
  theme(plot.title = element_text(hjust = 0.5))
missingValuePlot_test <- missingValues_test %>%
  ggplot(aes(x = valueType, y = n, fill = isMissing)) +
  geom_bar(position = "stack", stat = "identity") +
  labs(
    x = "Variable Name",
    y = "Count",
    title = "Testing Set Missing Value Count"
  ) +
  theme(plot.title = element_text(hjust = 0.5))

missingValue_plot <- grid.arrange(missingValuePlot_train, missingValuePlot_test, nrow = 1)
ggsave(file = "../Output/Images/missing value plot.png", missingValue_plot, bg = "white")
```

### Target Distribution

```{r}
trainsetRatio <- trainset %>%
  mutate(total = n()) %>%
  group_by(target, total) %>%
  summarise(count = n()) %>%
  mutate(ratio = round(count / total, 2)) %>%
  select(target, count, total, ratio) %>%
  ungroup()
pie_chart <- trainsetRatio %>%
  mutate(
    target = case_when(
      target == 1 ~ "Disaster",
      T ~ "Not Disaster"
    )
  ) %>%
  ggplot(aes(x = '', y = count, fill = target)) +
  geom_bar(stat = "identity") +
  geom_text(
    aes(
      y = count / 2 + c(0, cumsum(count)[-length(count)]),
      label = paste(percent(count / total), "(", count, ")", sep = '')
    ),
    size = 5
  ) +
  coord_polar("y", start = 0) +
  theme_minimal() +
  labs(
    title = "Target Distribution in Training Set",
    fill = "Target"
  ) +
  theme(
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    panel.border = element_blank(),
    panel.grid = element_blank(),
    axis.ticks = element_blank(),
    plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
    axis.text.x = element_blank()
  )

targetDistribution_plot <- grid.arrange(pie_chart, nrow = 1)
ggsave(file = "../Output/Images/target distribution plot.png", targetDistribution_plot, bg = "white")
```

## Data Cleaning

### Mislabeled Data

#### Mislabeled data in train set

```{r}
trainset %>%
  group_by(text, target) %>%
  count() %>%
  ungroup(target) %>%
  mutate(count = sum(n)) %>%
  filter(n != count) %>%
  arrange(text)
```

#### Mislabeled data in test set

```{r}
testset %>%
  group_by(text, target) %>%
  count() %>%
  ungroup(target) %>%
  mutate(count = sum(n)) %>%
  filter(n != count) %>%
  arrange(text)
```

#### Clean mislabeled data

```{r}
trainset <- trainset %>%
  mutate(target = case_when(
    str_detect(text, "#Allah describes piling") ~ as.integer(0),
    str_detect(text, "#foodscare #offers2go #NestleIndia") ~ as.integer(0),
    str_detect(text, ".POTUS #StrategicPatience") ~ as.integer(1),
    str_detect(text, "Caution: breathing") ~ as.integer(1),
    str_detect(text, "CLEARED:incident with") ~ as.integer(1),
    str_detect(text, "He came to a land which") ~ as.integer(0),
    str_detect(text, "Hellfire is surrounded by") ~ as.integer(0),
    str_detect(text, "Hellfire! We don") ~ as.integer(0),
    str_detect(text, "I Pledge Allegiance") ~ as.integer(0),
    str_detect(text, "In #islam saving") ~ as.integer(0),
    str_detect(text, "like for the music") ~ as.integer(0),
    str_detect(text, "Mmmmmm I'm burning") ~ as.integer(0),
    str_detect(text, "RT NotExplained") ~ as.integer(1),
    str_detect(text, "that horrible sinking") ~ as.integer(0),
    str_detect(text, "The Prophet") ~ as.integer(0),
    str_detect(text, "To fight bioterrorism") ~ as.integer(0),
    str_detect(text, "Who is bringing the") ~ as.integer(1),
    str_detect(text, "wowo--=== 12000") ~ as.integer(0),
    T ~ target
  ))
testset <- testset %>%
  mutate(target = case_when(
    str_detect(text, "To fight bioterrorism") ~ as.integer(0),
    str_detect(text, "that exploded") ~ as.integer(0),
    T ~ target
  ))
```

### Extract links from text

```{r}
extract_links <- function(dataset){
  dataset %>%
    mutate(
      links = map_chr(
        map(
          .x = str_extract_all(.$text, 'http\\S+\\.\\S+/\\S\\s*'),
          unlist
        ),
        ~case_when(
          paste(.x, collapse = ',') == '' ~ NA_character_,
          T ~ paste(.x, collapse = ',')
        )
      ),
      link_count = str_count(.$text, 'http\\S+\\.\\S+/\\S\\s*'),
      text = str_replace_all(
        text,
        "http\\S+//\\S+\\.?\\S+\\S\\s*",
        ''
      )
    )
}
tidy_train <- trainset %>%
  extract_links()
tidy_test <- testset %>%
  extract_links()
tidy_train
```

### Convert special url character (*%20* as white space) to readable string in each variable and replace newline character \n in text

```{r}
replace_url_special_character <- function(dataset){
  dataset %>%
    mutate(
      keyword = URLdecode(keyword)
    ) %>%
    mutate(
      keyword = na_if(keyword, "")
    )
}
replace_new_line <- function(dataset){
  dataset %>% mutate(
    line_count = str_count(text, "\n") + 1,
    text = str_replace_all(text, "\n", " ")
  )
}
tidy_train <- tidy_train %>%
  replace_url_special_character() %>%
  replace_new_line()
tidy_train

tidy_test <- tidy_test %>%
  replace_url_special_character() %>%
  replace_new_line()
```

### Extract Tags and '\@'s in *text* variable

```{r}
extract_at <- function(dataset){
  dataset %>%
    mutate(
      at = str_replace_all(
        map_chr(
          map(
            .x = str_extract_all(tolower(.$text), "@\\S+"),
            unlist
          ),
          ~case_when(
            paste(.x, collapse = ',') == '' ~ NA_character_,
            T ~ paste(.x, collapse = ',')
          )
        ),
        '@',
        ''
      ),
      at_count = str_count(tolower(.$text), "@\\S+"),
      text = str_replace_all(text, "@\\S+", '')
    )
}
extract_tags <- function(dataset){
  dataset %>%
    mutate(
      tags = str_replace_all(
        map_chr(
          map(
            .x = str_extract_all(tolower(.$text), "#[:alpha:][:alnum:]*"),
            unlist
          ),
          ~case_when(
            paste(.x, collapse = ',') == '' ~ NA_character_,
            T ~ paste(.x, collapse = ',')
          )
        ),
        '#',
        ''
      ),
      tag_count = str_count(tolower(.$text), "#\\S+"),
      text = str_replace_all(text, "#", '')
    )
}
tidy_train <- tidy_train %>%
  extract_tags() %>%
  extract_at()
tidy_train

tidy_test <- tidy_test %>%
  extract_tags() %>%
  extract_at()
```

### Parse *location* variable -- Situations in American

```{r warning=FALSE}
withLocation <- tidy_train %>%
  mutate(location = str_replace_all(location, regex("[^, [:alpha:]]+"), "")) %>%
  mutate(location = str_replace_all(location, regex("( )*,( )*"), ", ")) %>%
  filter(location != '') %>%
  separate(location, c('location 1', 'location 2'), ', ') %>%
  filter(!(`location 1` %in% c('', NA)) | !(`location 2`%in% c('', NA)))

withoutLocation <- tidy_train %>% anti_join(withLocation, by='id')

USAstate = cbind(tolower(state.abb), tolower(state.name)) %>%
  as_tibble() %>%
  rename(abb = V1, region = V2)

USAstate1_1 <- cbind(tolower(state.abb), tolower(state.name)) %>%
  as_tibble() %>%
  rename(abb1 = V1, 'location 1' = V2)
USAstate1_2 <- cbind(tolower(state.abb), tolower(state.abb)) %>%
  as_tibble() %>%
  rename(abb1 = V1, 'location 1' = V2)
USAstate2_1 <- cbind(tolower(state.abb), tolower(state.name)) %>%
  as_tibble() %>%
  rename(abb2 = V1, 'location 2' = V2)

withLocation$`location 1` <- tolower(withLocation$`location 1`)
withLocation$`location 2` <- tolower(withLocation$`location 2`)

withLocation <- withLocation %>% left_join(USAstate1_1, by = 'location 1')
withLocation <- withLocation %>% left_join(USAstate1_2, by = 'location 1')
withLocation <- withLocation %>% left_join(USAstate2_1, by = 'location 2')
withLocation[is.na(withLocation)] = ''

inUSA <- withLocation %>% filter(!((abb1.x=='') & (abb1.y=='') & (abb2==''))) %>% mutate(abb2 = ifelse(id == 7318, '', abb2))
inUSA[is.na(inUSA)] = ''
inUSA <- inUSA %>% unite(col = 'abb',c(abb1.x,abb1.y,abb2),sep = '',remove = TRUE)
inUSA <- inUSA %>% select(abb) %>% count(abb)
inUSA <- inUSA %>% left_join(USAstate, by = "abb") %>% select(region, n)
inUSA <- inUSA %>% add_row(region='district of columbia', n=0, .before = 9) %>% rename('region' = region, 'value' = n)

library(choroplethr)
library(choroplethrMaps)
disasterMap <- state_choropleth(title = "USA Disaster Map", inUSA) +
  theme(plot.title = element_text(hjust = 0.5))
disasterMap
ggsave("../Output/Images/disaster map.png", disasterMap, bg = "white")
```

## Topic Modeling with LDA

### Tokenization and Data cleaning

```{r}
tidy_trainset2 <- tidy_train
tidy_trainset2$text <- sub("RT.*:", "", tidy_trainset2$text)
text_cleaning_tokens <- tidy_trainset2 %>%
  select(id, text, target) %>%
  tidytext::unnest_tokens(word, text)
text_cleaning_tokens$word <- gsub('[[:digit:]]+', "", text_cleaning_tokens$word)
text_cleaning_tokens$word <- gsub('[[:punct:]]+', "", text_cleaning_tokens$word)
text_cleaning_tokens$word  <-  gsub("amp", "", text_cleaning_tokens$word) 
text_cleaning_tokens$word  <-  gsub("[\r]", "", text_cleaning_tokens$word)
text_cleaning_tokens <- text_cleaning_tokens %>%
  filter(!(nchar(word) == 1))%>% 
  anti_join(stop_words)
tokens <- text_cleaning_tokens %>%
  filter(!(word==""))
tokens <- tokens %>%
  mutate(ind = row_number())
tokens <- tokens %>%
  group_by(id) %>%
  mutate(ind = row_number()) %>%
  tidyr::spread(key = ind, value = word)
tokens [is.na(tokens)] <- ""
tokens <- tidyr::unite(tokens, text,-id,sep =" " )
tokens$text <- trimws(tokens$text)
```

### Model Tuning: Choosing the Best Model

```{r}
library(topicmodels)
library(textmineR)
```

```{r warning=FALSE}
dtm <- CreateDtm(
  tokens$text,
  doc_names = tokens$ID,
  ngram_window = c(1, 2)
)
# explore the basic frequency
tf <- TermDocFreq(dtm = dtm)
original_tf <- tf %>% select(term, term_freq,doc_freq)
rownames(original_tf) <- 1:nrow(original_tf)
# Eliminate words appearing less than 2 times or in more than half of the documents
vocabulary <- tf$term[ tf$term_freq > 1 & tf$doc_freq < nrow(dtm) / 2 ]
k_list <- seq(1, 20, by = 1)
lda_dir <- "../Output/LDA"
model_dir <- paste0(lda_dir,"/models_", digest::digest(vocabulary, algo = "sha1"))
if(!dir.exists(lda_dir)){
  dir.create(lda_dir)
}
if (!dir.exists(model_dir)){
  dir.create(model_dir)
}
model_list <- TmParallelApply(X = k_list, FUN = function(k){
  filename = file.path(model_dir, paste0(k, "_topics.rda"))
  
  if (!file.exists(filename)) {
    m <- FitLdaModel(dtm = dtm, k = k, iterations = 500)
    m$k <- k
    m$coherence <- CalcProbCoherence(phi = m$phi, dtm = dtm, M = 5)
    save(m, file = filename)
  }
  else {
    load(filename)
  }
  
  m
}, export=c("dtm", "model_dir")) 

coherence_mat <- data.frame(
  k = sapply(model_list, function(x) nrow(x$phi)),
  coherence = sapply(model_list, function(x) mean(x$coherence)),
  stringsAsFactors = FALSE
)

coherence_plot <- coherence_mat %>%
ggplot(aes(x = k, y = coherence)) +
  geom_point() +
  geom_line(group = 1) +
  labs(
    title = "Best Topic by Coherence Score",
    y = "Coherence",
    x = "K Value"
  ) +
  theme(plot.title = element_text(hjust = 0.5)) +
  scale_x_continuous(breaks = seq(1,20,1))
coherence_plot
ggsave(file = "../Output/Images/coherence plot.png", coherence_plot, bg = "white")
```

### Topics

ggsave(file = "../Output/Images/topic plot.png", topic_plot, bg = "white", dpi = 500)

```{r}
tweets_lda <- LDA(dtm, k = 20, method="Gibbs") # This might take a while
tweets_topics <- tidy(tweets_lda, matrix = "beta")

tweets_top_terms <- tweets_topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>% 
  ungroup() %>%
  arrange(topic, -beta)
topic_plot <- tweets_top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()
topic_plot
ggsave(file = "../Output/Images/topic plot.png", topic_plot, bg = "white", width = 14, height = 6)
```

## Word Cloud

```{r}
library(wordcloud2)
library(htmlwidgets)
library(webshot)
webshot::install_phantomjs()
```

### All words

```{r}
set.seed(1234)
wordcloud_all <- text_cleaning_tokens %>%
  filter(word != '') %>%
  anti_join(stop_words, copy = T) %>%
  count(word, sort=TRUE) %>%
  filter(n >= 20) %>%
  wordcloud2(size = 0.5)
wordcloud_all

saveWidget(wordcloud_all, "../Output/Images/wordcloud all.html", selfcontained = F)
webshot("../Output/Images/wordcloud all.html", "../Output/Images/wordcloud all.png", delay = 5, vwidth = 600, vheight = 480)
```

### Disaster word cloud

```{r}
wordcloud_disaster <- text_cleaning_tokens %>%
  filter(word != '') %>%
  anti_join(stop_words, copy = T) %>%
  filter(target == 1) %>%
  count(word, sort=TRUE) %>%
  filter(n >= 20) %>%
  wordcloud2(size = 0.5)
wordcloud_disaster

saveWidget(wordcloud_disaster, "../Output/Images/wordcloud disaster.html", selfcontained = F)
webshot("../Output/Images/wordcloud disaster.html", "../Output/Images/wordcloud disaster.png", delay = 5, vwidth = 600, vheight = 480)
```

## Feature Engineering

### Keyword

#### Ratio of each keyword vs target

```{r}
keyword_ratio <- tidy_train %>%
  filter(!is.na(keyword)) %>%
  group_by(keyword, target) %>%
  summarise(n = n()) %>%
  mutate(ratio = n / sum(n)) %>%
  ungroup() %>%
  arrange(desc(n), desc(ratio))
keyword_ratio
```

#### Create disaster keyword and non-disaster keyword

```{r}
count_threshold <- 20
ratio_threshold <- 0.8
disaster_keywords <- keyword_ratio %>%
  filter(n >= count_threshold & target == 1 & ratio >= ratio_threshold) %>%
  .$keyword
non_disaster_keywords <- keyword_ratio %>%
  filter(n >= count_threshold & target == 0 & ratio >= ratio_threshold) %>%
  .$keyword
disaster_keywords %>%
  as_tibble()
non_disaster_keywords %>%
  as_tibble()
```

```{r}
checkDisasterKeyword <- function(dataset, disaster_keywords){
  dataset %>%
    mutate(
      disaster_keyword_count = str_detect(
        keyword,
        paste(disaster_keywords, collapse = "|")
      )
    ) %>%
    mutate(disaster_keyword_count = case_when(
      disaster_keyword_count == T ~ as.integer(1),
      T ~ as.integer(0)
    ))
}
checkNonDisasterKeyword <- function(dataset, non_disaster_keywords){
  dataset %>%
    mutate(
      non_disaster_keyword_count = str_detect(
        keyword,
        paste(non_disaster_keywords, collapse = "|")
      )
    ) %>%
    mutate(non_disaster_keyword_count = case_when(
       non_disaster_keyword_count == T ~ as.integer(1),
      T ~ as.integer(0)
    ))
}

tidy_train <- tidy_train %>%
  checkDisasterKeyword(disaster_keywords) %>%
  checkNonDisasterKeyword(non_disaster_keywords)
tidy_test <- tidy_test %>%
  checkDisasterKeyword(disaster_keywords) %>%
  checkNonDisasterKeyword(non_disaster_keywords)
tidy_train
```

### Tag

#### Ratio of each tags vs target

```{r}
tag_ratio <- tidy_train %>%
  filter(!is.na(tags)) %>%
  separate_rows(tags, sep = ',') %>%
  group_by(tags, target) %>%
  summarise(n = n()) %>%
  mutate(ratio = n / sum(n)) %>%
  ungroup() %>%
  arrange(desc(n), desc(ratio), tags)
tag_ratio
```

#### Create disaster tag and non-disaster tag

```{r}
count_threshold <- 5
ratio_threshold <- 0.7
disaster_tag <- tag_ratio %>%
  filter(target == 1 & ratio >= ratio_threshold & n >= count_threshold) %>%
  .$tags
non_disaster_tag <- tag_ratio %>%
  filter(target == 0 & ratio >= ratio_threshold & n >= count_threshold) %>%
  .$tags
disaster_tag %>%
  as_tibble()
non_disaster_tag %>%
  as_tibble()
```

```{r}
countDisasterTag <- function(dataset, disaaster_tag){
   dataset %>%
    mutate(
      disaster_tag_count = str_count(
        tags,
        paste(
          "(",
          str_replace_all(
            paste(disaster_tag, collapse = "|"),
            '\\?',
            '\\\\?'
          ),
          ")",
          sep = ""
        )
      )
    ) %>%
    mutate(
      disaster_tag_count = case_when(
        is.na(disaster_tag_count) ~ as.integer(0),
        T ~ disaster_tag_count
      )
    )
}
countNonDisasterTag <- function(dataset, non_disaster_tag){
  dataset %>%
    mutate(
      non_disaster_tag_count = str_count(
        tags,
        paste(
          "(",
          str_replace_all(
            paste(non_disaster_tag, collapse = "|"),
            '\\?',
            '\\\\?'
          ),
          ")",
          sep = ""
        )
      )
    ) %>%
    mutate(
      non_disaster_tag_count = case_when(
        is.na(non_disaster_tag_count) ~ as.integer(0),
        T ~ non_disaster_tag_count
      )
    )
}
tidy_train <- tidy_train %>%
  countDisasterTag(disaster_tag) %>%
  countNonDisasterTag(non_disaster_tag)
  # checkDisasterTag(disaster_tag)
tidy_test <- tidy_test %>%
  countDisasterTag(disaster_tag) %>%
  countNonDisasterTag(non_disaster_tag)
  # checkDisasterTag(disaster_tag)
tidy_train
```

### At

#### Ratio of each at vs target

```{r}
at_ratio <- tidy_train %>%
  filter(!is.na(at)) %>%
  separate_rows(at, sep = ',') %>%
  group_by(at, target) %>%
  summarise(n = n()) %>%
  mutate(ratio = n / sum(n)) %>%
  arrange(desc(n), desc(ratio), at)
at_ratio
```

#### Create disaster at and non-disaster at

```{r}
count_threshold <- 5
ratio_threshold <- 0.7
disaster_at <- at_ratio %>%
  filter(target == 1 & ratio >= ratio_threshold & n >= count_threshold) %>%
  .$at
non_disaster_at <- at_ratio %>%
  filter(target == 0 & ratio >= ratio_threshold & n >= count_threshold) %>%
  .$at

countDisasterAt <- function(dataset, disaster_at){
  dataset %>%
    mutate(
      disaster_at_count = str_count(
        at,
        paste(
          "(",
          str_replace_all(
            paste(disaster_at, collapse = "|"),
            '\\?',
            '\\\\?'
          ),
          ")",
          sep = ""
        )
      )
    ) %>%
    mutate(
      disaster_at_count = case_when(
        is.na(disaster_at_count) ~ as.integer(0),
        T ~ disaster_at_count
      )
    )
}
countNonDisasterAt <- function(dataset, non_disaster_at){
  dataset %>%
    mutate(
      non_disaster_at_count = str_count(
        at,
        paste(
          "(",
          str_replace_all(
            paste(non_disaster_at, collapse = "|"),
            '\\?',
            '\\\\?'
          ),
          ")",
          sep = ""
        )
      )
    ) %>%
    mutate(
      non_disaster_at_count = case_when(
        is.na(non_disaster_at_count) ~ as.integer(0),
        T ~ non_disaster_at_count
      )
    )
}

tidy_train <- tidy_train %>%
  countDisasterAt(disaster_at) %>%
  countNonDisasterAt(non_disaster_at)
  # checkDisasterAt(disaster_at)
tidy_test <- tidy_test %>%
  countDisasterAt(disaster_at) %>%
  countNonDisasterAt(non_disaster_at)
  # checkDisasterAt(tidy_test, disaster_at)
tidy_train
```

### Text

#### Extract *char length*, *word count*, *mean word count*, *unique word count*, *punctuation count*, *stop words count*

```{r}
extract_char_length <- function(dataset){
  dataset %>%
    mutate(char_length = nchar(.$text))
}
extract_word_count <- function(dataset){
  dataset %>%
    mutate(word_count = str_count(text, "\\w+"))
}
extract_mean_word_length <- function(dataset){
  dataset %>%
    mutate(mean_word_length = char_length/word_count)
}
extract_unique_word_count <- function(dataset){
  dataset %>%
    mutate(
      unique_word_count = map_int(
        .$text,
        ~ .x %>% str_extract_all("\\w+") %>% unlist %>% n_distinct
      )
    )
}
extract_punctuation_count <- function(dataset){
  dataset %>%
    mutate(
      punc_count = str_count(text, "[[:punct:]]+")
    )
}
extract_stop_word_count <- function(dataset){
  dataset %>%
  mutate(
    stop_word_count = map_int(
      text,
      ~ .x %>%
        tolower() %>%
        str_split(' ') %>%
        map(., ~case_when(. %in% stop_words$word ~ T, T ~ F)) %>%
        unlist %>%
        sum()
    )
  )
}

# classify_char_length <- function(dataset){
#   dataset %>%
#     mutate(
#       char_length_type = case_when(
#         char_length <= 50 ~ "short",
#         char_length <= 100 ~ "medium",
#         char_length <= 150 ~ "long",
#         T ~ "other"
#       )
#     )
# }
# classify_word_length <- function(dataset){
#   dataset %>%
#     mutate(
#       word_count_type = case_when(
#         word_count <= 5 ~ "0-5",
#         word_count <= 10 ~ "5-10",
#         word_count <= 15 ~ "10-15",
#         word_count <= 20 ~ "15-20",
#         word_count <= 25 ~ "20-25",
#         word_count <= 30 ~ "25-30",
#         T ~ "other"
#       )
#     )
# }

tidy_train <- tidy_train %>%
  extract_char_length() %>%
  extract_word_count() %>%
  extract_mean_word_length() %>%
  extract_unique_word_count() %>%
  extract_punctuation_count() %>%
  extract_stop_word_count()

tidy_test <- tidy_test %>%
  extract_char_length() %>%
  extract_word_count() %>%
  extract_mean_word_length() %>%
  extract_unique_word_count() %>%
  extract_punctuation_count() %>%
  extract_stop_word_count()

# tidy_train <- tidy_train %>%
#   classify_char_length() %>%
#   classify_word_length()
# 
# tidy_test <- tidy_test %>%
#   classify_char_length() %>%
#   classify_word_length()

tidy_train
```

#### Distribution of each feature

```{r}
char_length_distribution <- tidy_train %>%
  mutate(target = as.factor(target)) %>%
  ggplot() +
  geom_histogram(aes(x = char_length, fill = as.factor(target)), binwidth = 10) +
  labs(
    x = "Char Length",
    y = "Count",
    title = "Dist. of Char Length in Train Set",
    fill = "Target"
  ) +
  theme(plot.title = element_text(hjust = 0.5))

word_count_distribution <- tidy_train %>%
  mutate(target = as.factor(target)) %>%
  ggplot() +
  geom_histogram(aes(x = word_count, fill = as.factor(target)), binwidth = 5) +
  labs(
    x = "Word Count",
    y = "Count",
    title = "Dist. of Word Count in Train Set",
    fill = "Target"
  ) +
  theme(plot.title = element_text(hjust = 0.5))

mean_word_length_distribution <- tidy_train %>%
  mutate(target = as.factor(target)) %>%
  ggplot() +
  geom_histogram(aes(x = mean_word_length, fill = as.factor(target)), binwidth = 1) +
  labs(
    x = "Mean Word Length",
    y = "Count",
    title = "Dist. of Mean Word Length in Train Set",
    fill = "Target"
  ) + 
  theme(plot.title = element_text(hjust = 0.5))
unique_word_count_distribution <- tidy_train %>%
  mutate(target = as.factor(target)) %>%
  ggplot() +
  geom_histogram(aes(x = unique_word_count, fill = as.factor(target)), binwidth = 5) +
  labs(
    x = "Unique Word Count",
    y = "Count",
    title = "Dist. of Unique Word Count in Train Set",
    fill = "Target"
  ) +
  theme(plot.title = element_text(hjust = 0.5))

punctuation_count_distribution <- tidy_train %>%
  mutate(target = as.factor(target)) %>%
  ggplot() +
  geom_histogram(aes(x = punc_count, fill = as.factor(target)), binwidth = 1) +
  labs(
    x = "Punctuation Count",
    y = "Count",
    title = "Dist. of Punctuation Count in Train Set",
    fill = "Target"
  ) +
  theme(plot.title = element_text(hjust = 0.5))

stop_word_count_distribution <- tidy_train %>%
  mutate(target = as.factor(target)) %>%
  ggplot() + 
  geom_histogram(aes(x = stop_word_count, fill = as.factor(target)), binwidth = 5) +
  labs(
    x = "Stop Word Count",
    y = "Count",
    title = "Dist. of Stop Word Count in Train Set",
    fill = "Target"
  ) + 
  theme(plot.title = element_text(hjust = 0.5))

distribution_plot <- grid.arrange(
  char_length_distribution,
  word_count_distribution,
  mean_word_length_distribution,
  unique_word_count_distribution,
  punctuation_count_distribution,
  stop_word_count_distribution,
  ncol = 2
)
ggsave(file = "../Output/Images/distribution plot.png", distribution_plot, bg = "white")
```

#### N-gram analysis

##### Unigram

```{r}
unigrams <- tidy_train %>%
  unnest_tokens(unigram, text, token = "ngrams", n = 1) %>%
  filter(unigram != "NA") %>%
  filter(str_detect(unigram, "^[a-z]+$")) %>%
  filter(!(unigram %in% stop_words$word)) %>%
  group_by(unigram, target) %>%
  count() %>%
  ungroup(target) %>%
  mutate(count = sum(n)) %>%
  mutate(ratio = n/count) %>%
  arrange(desc(n), desc(ratio)) %>%
  ungroup()
unigrams
```

##### Disaster unigram and non-disaster unigram

```{r}
count_threshold <- 25
ratio_threshold <- 0.65
disaster_unigrams <- unigrams %>%
  filter(target == 1 & n >= count_threshold & ratio >= ratio_threshold) %>%
  arrange(desc(n)) %>%
  select(unigram, n)

non_disaster_unigrams <- unigrams %>%
  filter(target == 0 & n >= count_threshold & ratio >= ratio_threshold) %>%
  arrange(desc(n)) %>%
  select(unigram, n)

top20DisasterUnigrams_plot <- disaster_unigrams %>%
  head(20) %>%
  ggplot(aes(reorder(unigram, n), n)) +
  geom_bar(stat = "identity") + 
  coord_flip() +
  labs(
    x = "Unigram",
    y = "Count",
    title = "Top 20 Disaster Unigrams"
  ) + 
  theme(plot.title = element_text(hjust = 0.5))

top20NonDisasterUnigrams_plot <- non_disaster_unigrams %>%
  head(20) %>%
  ggplot(aes(reorder(unigram, n), n)) +
  geom_bar(stat = "identity") + 
  coord_flip() +
  labs(
    x = "Unigram",
    y = "Count",
    title = "Top 20 Non-Disaster Unigrams"
  ) + 
  theme(plot.title = element_text(hjust = 0.5))

top20Unigrams_plot <- grid.arrange(top20DisasterUnigrams_plot, top20NonDisasterUnigrams_plot, nrow = 1)
ggsave(file = "../Output/Images/top20Unigram plot.png", top20Unigrams_plot, bg = "white")
```

```{r}
countDisasterUnigram <- function(dataset, disaster_unigram){
  dataset %>%
    mutate(
      disaster_unigram_count = str_count(
        text,
        paste(
          "(",
          str_replace_all(
            paste(disaster_unigram, collapse = "|"),
            '\\?',
            '\\\\?'
          ),
          ")",
          sep = ""
        )
      )
    )
}
countNonDisasterUnigram <- function(dataset, non_disaster_unigram){
  dataset %>%
    mutate(
      non_disaster_unigram_count = str_count(
        text,
        paste(
          "(",
          str_replace_all(
            paste(non_disaster_unigram, collapse = "|"),
            '\\?',
            '\\\\?'
          ),
          ")",
          sep = ""
        )
      )
    )
}

tidy_train <- tidy_train %>%
  countDisasterUnigram(disaster_unigrams$unigram) %>%
  countNonDisasterUnigram(non_disaster_unigrams$unigram)
tidy_test <- tidy_test %>%
  countDisasterUnigram(disaster_unigrams$unigram) %>%
  countNonDisasterUnigram(non_disaster_unigrams$unigram)
tidy_train
```

##### Bigram

```{r}
bigrams <- tidy_train %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
  filter(bigram != "NA") %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>%
  filter(str_detect(word1, "^[a-z]+$")) %>%
  filter(str_detect(word2, "^[a-z]+$")) %>%
  filter(!(word1 %in% stop_words$word)) %>%
  filter(!(word2 %in% stop_words$word)) %>%
  unite(bigram, word1, word2, sep = " ") %>%
  group_by(bigram, target) %>%
  count() %>%
  ungroup(target) %>%
  mutate(count = sum(n)) %>%
  ungroup() %>%
  mutate(ratio = n/count) %>%
  arrange(desc(count), desc(ratio))
bigrams
```

##### Disaster bigram and non-disaster bigram

```{r}
count_threshold <- 10
ratio_threshold <- 0.65
disaster_bigrams <- bigrams %>%
  filter(target == 1 & n >= count_threshold & ratio >= ratio_threshold) %>%
  arrange(desc(n)) %>%
  select(bigram, n)

top20DisasterBigrams_plot <- disaster_bigrams %>%
  head(20) %>%
  ggplot(aes(reorder(bigram, n), n)) +
  geom_bar(stat = "identity") + 
  coord_flip() +
  labs(
    x = "Bigram",
    y = "Count",
    title = "Top 20 Disaster Bigrams"
  ) + 
  theme(plot.title = element_text(hjust = 0.5))

non_disaster_bigrams <- bigrams %>%
  filter(target == 0 & n >= count_threshold & ratio >= ratio_threshold) %>%
  arrange(desc(n)) %>%
  select(bigram, n)

top20NonDisasterBigrams_plot <- non_disaster_bigrams %>%
  head(20) %>%
  ggplot(aes(reorder(bigram, n), n)) +
  geom_bar(stat = "identity") + 
  coord_flip() +
  labs(
    x = "Bigram",
    y = "Count",
    title = "Top 20 Non-Disaster Bigrams"
  ) + 
  theme(plot.title = element_text(hjust = 0.5))

top20Bigram_plot <- grid.arrange(top20DisasterBigrams_plot, top20NonDisasterBigrams_plot, nrow = 1)
ggsave(file = "../Output/Images/top20Bigram plot.png", top20Bigram_plot, bg = "white")
```

```{r}
countDisasterBigram <- function(dataset, disaster_bigram){
  dataset %>%
    mutate(
      disaster_bigram_count = str_count(
        text,
        paste(
          "(",
          str_replace_all(
            paste(disaster_bigram, collapse = "|"),
            '\\?',
            '\\\\?'
          ),
          ")",
          sep = ""
        )
      )
    )
}
countNonDisasterBigram <- function(dataset, non_disaster_bigram){
  dataset %>%
    mutate(
      non_disaster_bigram_count = str_count(
        text,
        paste(
          "(",
          str_replace_all(
            paste(non_disaster_bigram, collapse = "|"),
            '\\?',
            '\\\\?'
          ),
          ")",
          sep = ""
        )
      )
    )
}

tidy_train <- tidy_train %>%
  countDisasterBigram(disaster_bigrams$bigram) %>%
  countNonDisasterBigram(non_disaster_bigrams$bigram)
tidy_test <- tidy_test %>%
  countDisasterBigram(disaster_bigrams$bigram) %>%
  countNonDisasterBigram(non_disaster_bigrams$bigram)
tidy_train
```

##### Trigram

```{r}
trigrams <- tidy_train %>%
  unnest_tokens(trigram, text, token = "ngrams", n = 3) %>%
  filter(trigram != "NA") %>%
  separate(trigram, c("word1", "word2", "word3"), sep = " ") %>%
  filter(str_detect(word1, "^[a-z]+$")) %>%
  filter(str_detect(word2, "^[a-z]+$")) %>%
  filter(str_detect(word3, "^[a-z]+$")) %>%
  filter(!(word1 %in% stop_words$word)) %>%
  filter(!(word2 %in% stop_words$word)) %>%
  filter(!(word3 %in% stop_words$word)) %>%
  unite(trigram, word1, word2, word3, sep = " ") %>%
  group_by(trigram, target) %>%
  count() %>%
  ungroup(target) %>%
  mutate(count = sum(n)) %>%
  mutate(ratio = n/count) %>%
  arrange(desc(count), desc(ratio))
trigrams
```

##### Disaster trigram and non-disaster trigram

```{r}
count_threshold <- 10
ratio_threshold <- 0.65
disaster_trigrams <- trigrams %>%
  filter(target == 1 & n >= count_threshold & ratio >= ratio_threshold) %>%
  arrange(desc(n)) %>%
  select(trigram, n)

top20DisasterTrigrams_plot <- disaster_trigrams %>%
  head(20) %>%
  ggplot(aes(reorder(trigram, n), n)) +
  geom_bar(stat = "identity") + 
  coord_flip() +
  labs(
    x = "Trigram",
    y = "Count",
    title = "Top 20 Disaster Trigrams"
  ) + 
  theme(plot.title = element_text(hjust = 0.5))

non_disaster_trigrams <- trigrams %>%
  filter(target == 0 & n >= count_threshold & ratio >= ratio_threshold) %>%
  arrange(desc(n)) %>%
  select(trigram, n)

top20NonDisasterTrigrams_plot <- non_disaster_trigrams %>%
  head(20) %>%
  ggplot(aes(reorder(trigram, n), n)) +
  geom_bar(stat = "identity") + 
  coord_flip() +
  labs(
    x = "Trigram",
    y = "Count",
    title = "Top 20 Non-Disaster Trigrams"
  ) + 
  theme(plot.title = element_text(hjust = 0.5))

top20Trigram_plot <- grid.arrange(top20DisasterTrigrams_plot, top20NonDisasterTrigrams_plot, nrow = 1)
ggsave(file = "../Output/Images/top20Trigram plot.png", top20Trigram_plot, bg = "white")
```

```{r}
countDisasterTrigram <- function(dataset, disaster_trigram){
  dataset %>%
    mutate(
      disaster_trigram_count = str_count(
        text,
        paste(
          "(",
          str_replace_all(
            paste(disaster_trigram, collapse = "|"),
            '\\?',
            '\\\\?'
          ),
          ")",
          sep = ""
        )
      )
    )
}
countNonDisasterTrigram <- function(dataset, non_disaster_trigram){
  dataset %>%
    mutate(
      non_disaster_trigram_count = str_count(
        text,
        paste(
          "(",
          str_replace_all(
            paste(non_disaster_trigram, collapse = "|"),
            '\\?',
            '\\\\?'
          ),
          ")",
          sep = ""
        )
      )
    )
}

tidy_train <- tidy_train %>%
  countDisasterTrigram(disaster_trigrams$trigram) %>%
  countNonDisasterTrigram(non_disaster_trigrams$trigram)
tidy_test <- tidy_test %>%
  countDisasterTrigram(disaster_trigrams$trigram) %>%
  countNonDisasterTrigram(non_disaster_trigrams$trigram)
tidy_train
```

### Sentimental Analysis

#### Lexicon

```{r}
nrc_hashtag <- read_table(
  '../Lexicon/NRC-Hashtag-Emotion-Lexicon-v0.2.txt',
  col_names = c('type', 'word', 'score')
) %>%
  mutate(word=str_replace_all(word, '#', ''))

nrc_hashtag <- rbind(
  nrc_hashtag %>% filter(type %in% c('fear', 'anger', 'sadness', 'disgust')) %>% mutate(score = -score),
  nrc_hashtag %>% filter(type %in% c('joy', 'anticipation', 'trust', 'surprise')) %>% mutate(score = score)
)
nrc_hashtag
```

#### Get sentimental score of each tweet

```{r}
getSentiments <- function(dataset, lexicon){
  dataset %>%
    unnest_tokens(word, text) %>%
    left_join(lexicon, by='word') %>%
    group_by(id) %>%
    summarise(score = sum(score, na.rm = T)) %>%
    right_join(dataset, by='id')
}

tidy_train <- tidy_train %>%
  getSentiments(nrc_hashtag)
tidy_test <- tidy_test %>%
  getSentiments(nrc_hashtag)
tidy_train
```

# Train-validation split based on target ratio

```{r}
total <- nrow(trainset)
zeroRatio <- trainsetRatio %>%
  filter(target == 0) %>%
  pull(ratio)
oneRatio <- trainsetRatio %>%
  filter(target == 1) %>%
  pull(ratio)

# set random seed for sample
set.seed(25)

# extract 20% data from trainset as validset
valRatio <- 0.2
zero <- tidy_train %>%
  filter(target == 0) %>%
  sample_n(zeroRatio * total * valRatio)
one <- tidy_train %>%
  filter(target == 1) %>%
  sample_n(oneRatio * total * valRatio)
tidy_valid <- bind_rows(
  sample_n(
    filter(tidy_train, target == 0),
    zeroRatio * total * valRatio
  ),
  sample_n(
    filter(tidy_train, target == 1),
    oneRatio * total * valRatio
  )
) %>%
  arrange(id)

# exclude those validset data from trainset
tidy_train <- tidy_train %>%
  anti_join(tidy_valid, by = "id") %>%
  arrange(id)

tidy_train
tidy_valid
```

# Model

## Random Forest Training

### Boosting

```{r}
library(gbm)
```

```{r}
tidy_train
```

```{r}
processModelInput <- function(dataset){
  dataset %>%
  select(
    target,
    link_count,
    line_count,
    tag_count,
    at_count,
    disaster_keyword_count,
    non_disaster_keyword_count,
    disaster_tag_count,
    non_disaster_tag_count,
    disaster_at_count,
    non_disaster_at_count,
    char_length,
    word_count,
    mean_word_length,
    unique_word_count,
    punc_count,
    stop_word_count,
    disaster_unigram_count,
    non_disaster_unigram_count,
    disaster_bigram_count,
    non_disaster_bigram_count,
    disaster_trigram_count,
    non_disaster_trigram_count,
    score
  )
}
input_train <- tidy_train %>%
  processModelInput()
input_train

input_valid <- tidy_valid %>%
  processModelInput()
input_test <- tidy_test %>%
  processModelInput()
```

```{r}
boosting <- gbm(
  as.character(target) ~.,
  data = input_train,,
  n.trees = 5000,
  distribution = "adaboost",
  interaction.depth = 3,
  shrinkage = 0.2
)
boosting
```

```{r}
predict.result <- predict(boosting, input_valid, n.trees = 320, type = 'response')
predict.result[predict.result > 0.5] = 1
predict.result[predict.result <= 0.5] = 0
predict.result %>%
  as.factor() %>%
  confusionMatrix(as.factor(input_valid$target))
```

### Bagging

```{r}
library(randomForest)
```

```{r}
bagging <- randomForest(
  as.factor(target) ~.,
  data = input_train,
  importance = T,
  ntree = 2000
)
bagging
```

```{r}
p1 <- predict(bagging, input_valid)
p1 %>%
  as.factor() %>%
  confusionMatrix(as.factor(input_valid$target))
```

## Neural Network Training

```{r}
library(neuralnet)
```

```{r}
set.seed(25)
network <- neuralnet(
    target ~
      link_count + 
      line_count +
      tag_count +
      at_count +
      disaster_keyword_count +
      non_disaster_keyword_count +
      disaster_tag_count +
      non_disaster_tag_count +
      disaster_at_count +
      non_disaster_at_count +
      char_length +
      word_count +
      mean_word_length +
      unique_word_count +
      punc_count +
      stop_word_count +
      disaster_unigram_count +
      non_disaster_unigram_count +
      disaster_bigram_count +
      non_disaster_bigram_count +
      disaster_trigram_count +
      non_disaster_trigram_count + 
      score
    ,
    data = input_train,
    rep = 1,
    hidden = 5,
    threshold = 0.06,
    linear.output = F,
    lifesign = "full",
    lifesign.step = 5000,
    act.fct = 'logistic',
    err.fct = 'ce',
    stepmax = 1000000,
    likelihood = T
  )
network$result.matrix[1,]
plot(network, rep = "best")
```

```{r}
network.results <- compute(network, input_valid, rep = which.min(network$result.matrix[1, ]))
results <- data.frame(actual = input_valid$target, prediction = round(network.results$net.result))
results
t <- table(results$actual, results$prediction)
t
prop.table(t)
sum(diag(t))/sum(t)
```

# Twitter API

```{r}
# We downloaded a lot of data from the twitter api using postman
data1 <- fromJSON('../Data/Twitter API Data/las.json')$data$text %>% as_tibble()
data2 <- fromJSON('../Data/Twitter API Data/DisasterInfoPH.json')$data$text %>% as_tibble()
data3 <- fromJSON('../Data/Twitter API Data/Disaster_Center.json')$data$text %>% as_tibble()
data4 <- fromJSON('../Data/Twitter API Data/earthquakeBot.json')$data$text %>% as_tibble()
data5 <- fromJSON('../Data/Twitter API Data/USGSted.json')$data$text %>% as_tibble()
data6 <- fromJSON('../Data/Twitter API Data/_GlobalCrisis_.json')$data$text %>% as_tibble()
data7 <- fromJSON('../Data/Twitter API Data/njdepforestfire.json')$data$text %>% as_tibble()
data8 <- fromJSON('../Data/Twitter API Data/FFMVic.json')$data$text %>% as_tibble()

disasterData <- rbind(data2, data3, data4, data5, data6, data7, data8) %>%
  mutate(target=1)
nonDisasterData <- data1 %>% mutate(target=0)
twitterData <- rbind(disasterData, nonDisasterData)


twitterData <- twitterData %>%
  mutate(id = row_number()) %>%
  rename(c('text' = value))
twitterData
```
